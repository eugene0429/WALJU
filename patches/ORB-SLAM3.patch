diff --git a/CMakeLists.txt b/CMakeLists.txt
index 016e743..3e1c438 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -12,20 +12,20 @@ set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wall   -O3")
 set(CMAKE_C_FLAGS_RELEASE "${CMAKE_C_FLAGS_RELEASE} -march=native")
 set(CMAKE_CXX_FLAGS_RELEASE "${CMAKE_CXX_FLAGS_RELEASE} -march=native")
 
-# Check C++11 or C++0x support
+# Check C++14 support (required for Pangolin's sigslot)
 include(CheckCXXCompilerFlag)
+CHECK_CXX_COMPILER_FLAG("-std=c++14" COMPILER_SUPPORTS_CXX14)
 CHECK_CXX_COMPILER_FLAG("-std=c++11" COMPILER_SUPPORTS_CXX11)
-CHECK_CXX_COMPILER_FLAG("-std=c++0x" COMPILER_SUPPORTS_CXX0X)
-if(COMPILER_SUPPORTS_CXX11)
+if(COMPILER_SUPPORTS_CXX14)
+   set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++14")
+   add_definitions(-DCOMPILEDWITHC14)
+   message(STATUS "Using flag -std=c++14.")
+elseif(COMPILER_SUPPORTS_CXX11)
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++11")
    add_definitions(-DCOMPILEDWITHC11)
    message(STATUS "Using flag -std=c++11.")
-elseif(COMPILER_SUPPORTS_CXX0X)
-   set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++0x")
-   add_definitions(-DCOMPILEDWITHC0X)
-   message(STATUS "Using flag -std=c++0x.")
 else()
-   message(FATAL_ERROR "The compiler ${CMAKE_CXX_COMPILER} has no C++11 support. Please use a different C++ compiler.")
+   message(FATAL_ERROR "The compiler ${CMAKE_CXX_COMPILER} has no C++14/C++11 support. Please use a different C++ compiler.")
 endif()
 
 LIST(APPEND CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake_modules)
@@ -172,6 +172,22 @@ add_executable(stereo_euroc
         Examples/Stereo/stereo_euroc.cc)
 target_link_libraries(stereo_euroc ${PROJECT_NAME})
 
+add_executable(stereo_euroc_realtime
+        Examples/Stereo/stereo_euroc_realtime.cc)
+target_link_libraries(stereo_euroc_realtime ${PROJECT_NAME})
+
+add_executable(stereo_realtime
+        Examples/Stereo/stereo_realtime.cc)
+target_link_libraries(stereo_realtime ${PROJECT_NAME})
+
+add_executable(stereo_custom
+        Examples/Stereo/stereo_custom.cc)
+target_link_libraries(stereo_custom ${PROJECT_NAME})
+
+add_executable(stereo_custom_dataset
+        Examples/Stereo/stereo_custom_dataset.cc)
+target_link_libraries(stereo_custom_dataset ${PROJECT_NAME})
+
 add_executable(stereo_tum_vi
         Examples/Stereo/stereo_tum_vi.cc)
 target_link_libraries(stereo_tum_vi ${PROJECT_NAME})
diff --git a/Examples/Stereo/Custom.yaml b/Examples/Stereo/Custom.yaml
new file mode 100644
index 0000000..337789c
--- /dev/null
+++ b/Examples/Stereo/Custom.yaml
@@ -0,0 +1,112 @@
+%YAML:1.0
+
+#--------------------------------------------------------------------------------------------
+# Custom Camera Configuration
+# Camera intrinsics: fx=fy=610.17784, cx=cy=512, baseline=0.31m
+#--------------------------------------------------------------------------------------------
+
+#--------------------------------------------------------------------------------------------
+# System config
+#--------------------------------------------------------------------------------------------
+
+# When the variables are commented, the system doesn't load a previous session or not store the current one
+
+# If the LoadFile doesn't exist, the system give a message and create a new Atlas from scratch
+#System.LoadAtlasFromFile: "Session_Custom_Stereo"
+
+# The store file is created from the current session, if a file with the same name exists it is deleted
+#System.SaveAtlasToFile: "Session_Custom_Stereo"
+
+#--------------------------------------------------------------------------------------------
+# Camera Parameters
+#--------------------------------------------------------------------------------------------
+File.version: "1.0"
+
+Camera.type: "PinHole"
+
+# Left Camera calibration and distortion parameters (OpenCV)
+# Assuming no distortion - adjust if you have distortion coefficients
+Camera1.fx: 610.17784
+Camera1.fy: 610.17784
+Camera1.cx: 512.0
+Camera1.cy: 512.0
+
+# Distortion coefficients (k1, k2, p1, p2, k3)
+# Set to 0 if no distortion information available
+Camera1.k1: 0.0
+Camera1.k2: 0.0
+Camera1.p1: 0.0
+Camera1.p2: 0.0
+
+# Right Camera calibration (assuming same as left for rectified stereo)
+Camera2.fx: 610.17784
+Camera2.fy: 610.17784
+Camera2.cx: 512.0
+Camera2.cy: 512.0
+
+Camera2.k1: 0.0
+Camera2.k2: 0.0
+Camera2.p1: 0.0
+Camera2.p2: 0.0
+
+# Image resolution (adjust to your actual image size)
+# Common sizes: 1024x1024, 640x480, 1280x720, 1920x1080
+Camera.width: 1024
+Camera.height: 1024
+
+# Camera frames per second (adjust based on your timestamp intervals)
+# From your data: timestamps are ~50ms apart = 20 FPS
+Camera.fps: 20
+
+# Color order of the images (0: BGR, 1: RGB. It is ignored if images are grayscale)
+Camera.RGB: 1
+
+# Stereo baseline times fx (for depth computation)
+# baseline = 0.31m, bf = baseline * fx = 0.31 * 610.17784 = 189.155
+Stereo.ThDepth: 40.0
+
+# Transformation from left to right camera (assuming rectified stereo pair)
+# For rectified stereo: only translation along X-axis (baseline)
+# T_c1_c2 = [R | t] where R = I (identity), t = [-baseline, 0, 0]
+Stereo.T_c1_c2: !!opencv-matrix
+  rows: 4
+  cols: 4
+  dt: f
+  data: [1.0, 0.0, 0.0, -0.31,
+         0.0, 1.0, 0.0, 0.0,
+         0.0, 0.0, 1.0, 0.0,
+         0.0, 0.0, 0.0, 1.0]
+
+#--------------------------------------------------------------------------------------------
+# ORB Parameters
+#--------------------------------------------------------------------------------------------
+
+# ORB Extractor: Number of features per image
+ORBextractor.nFeatures: 1200
+
+# ORB Extractor: Scale factor between levels in the scale pyramid 	
+ORBextractor.scaleFactor: 1.2
+
+# ORB Extractor: Number of levels in the scale pyramid	
+ORBextractor.nLevels: 8
+
+# ORB Extractor: Fast threshold
+# Image is divided in a grid. At each cell FAST are extracted imposing a minimum response.
+# Firstly we impose iniThFAST. If no corners are detected we impose a lower value minThFAST
+# You can lower these values if your images have low contrast			
+ORBextractor.iniThFAST: 20
+ORBextractor.minThFAST: 7
+
+#--------------------------------------------------------------------------------------------
+# Viewer Parameters
+#--------------------------------------------------------------------------------------------
+Viewer.KeyFrameSize: 0.05
+Viewer.KeyFrameLineWidth: 1.0
+Viewer.GraphLineWidth: 0.9
+Viewer.PointSize: 2.0
+Viewer.CameraSize: 0.08
+Viewer.CameraLineWidth: 3.0
+Viewer.ViewpointX: 0.0
+Viewer.ViewpointY: -0.7
+Viewer.ViewpointZ: -1.8
+Viewer.ViewpointF: 500.0
diff --git a/Examples/Stereo/CustomDataset.yaml b/Examples/Stereo/CustomDataset.yaml
new file mode 100644
index 0000000..6d33166
--- /dev/null
+++ b/Examples/Stereo/CustomDataset.yaml
@@ -0,0 +1,128 @@
+%YAML:1.0
+
+#--------------------------------------------------------------------------------------------
+# Custom Dataset Camera Configuration
+# Camera: OmniVision OV9782 Stereo
+# Camera coordinate: Z(forward), Y(up), X(right)
+# World/Body coordinate: Z-UP (ENU or similar)
+# Camera tilt: 20 degrees
+#--------------------------------------------------------------------------------------------
+
+#--------------------------------------------------------------------------------------------
+# System config
+#--------------------------------------------------------------------------------------------
+
+# When the variables are commented, the system doesn't load a previous session or not store the current one
+
+# If the LoadFile doesn't exist, the system give a message and create a new Atlas from scratch
+#System.LoadAtlasFromFile: "Session_CustomDataset_Stereo"
+
+# The store file is created from the current session, if a file with the same name exists it is deleted
+#System.SaveAtlasToFile: "Session_CustomDataset_Stereo"
+
+#--------------------------------------------------------------------------------------------
+# Camera Parameters
+#--------------------------------------------------------------------------------------------
+File.version: "1.0"
+
+Camera.type: "PinHole"
+
+# Left Camera calibration (OmniVision OV9782)
+# From camera matrix:
+# [921.6, 0, 512]
+# [0, 921.6, 512]
+# [0, 0, 1]
+Camera1.fx: 921.6
+Camera1.fy: 921.6
+Camera1.cx: 512.0
+Camera1.cy: 512.0
+
+# Distortion coefficients (k1, k2, p1, p2, k3)
+# Set to 0 if no distortion information available
+Camera1.k1: 0.0
+Camera1.k2: 0.0
+Camera1.p1: 0.0
+Camera1.p2: 0.0
+
+# Right Camera calibration (assuming same as left for rectified stereo)
+Camera2.fx: 921.6
+Camera2.fy: 921.6
+Camera2.cx: 512.0
+Camera2.cy: 512.0
+
+Camera2.k1: 0.0
+Camera2.k2: 0.0
+Camera2.p1: 0.0
+Camera2.p2: 0.0
+
+# Image resolution (1000x1000 based on cx=cy=500)
+Camera.width: 1024
+Camera.height: 1024
+
+# Camera frames per second (analyzed from dataset: ~0.1s interval = 10 FPS)
+Camera.fps: 10
+
+# Color order of the images (0: BGR, 1: RGB)
+Camera.RGB: 1
+
+# Stereo baseline times fx (for depth computation)
+# Assuming baseline ~0.4m (left camera x offset from body center * 2)
+# baseline = 0.48 * 2 = 0.96m approximately, or use actual stereo baseline
+# bf = baseline * fx = 0.4 * 703.125 = 281.25
+Stereo.ThDepth: 40.0
+
+# Transformation from left to right camera
+# Assuming rectified stereo pair with baseline along X-axis
+# Baseline estimated from left camera position (x=0.48m from body center)
+# Total stereo baseline ~ 0.4m (adjust if you have exact value)
+Stereo.T_c1_c2: !!opencv-matrix
+  rows: 4
+  cols: 4
+  dt: f
+  data: [1.0, 0.0, 0.0, -0.4,
+         0.0, 1.0, 0.0, 0.0,
+         0.0, 0.0, 1.0, 0.0,
+         0.0, 0.0, 0.0, 1.0]
+
+#--------------------------------------------------------------------------------------------
+# Camera-Body Transform (for reference)
+#--------------------------------------------------------------------------------------------
+# Left camera position in body frame:
+#   translation: x=0.48, y=0.2, z=1.145
+#   rotation: qx=-0.5792, qy=0.5792, qz=-0.4056, qw=0.4056
+# Camera coordinate: Z(forward), Y(up), X(right)
+# Body/World coordinate: Z-UP
+
+#--------------------------------------------------------------------------------------------
+# ORB Parameters - Optimized for Lunar Surface (low texture, low contrast)
+#--------------------------------------------------------------------------------------------
+
+# ORB Extractor: Number of features per image (increased for low-texture environment)
+ORBextractor.nFeatures: 2500
+
+# ORB Extractor: Scale factor between levels in the scale pyramid 	
+ORBextractor.scaleFactor: 1.2
+
+# ORB Extractor: Number of levels in the scale pyramid	
+ORBextractor.nLevels: 8
+
+# ORB Extractor: Fast threshold
+# Image is divided in a grid. At each cell FAST are extracted imposing a minimum response.
+# Firstly we impose iniThFAST. If no corners are detected we impose a lower value minThFAST
+# Lowered thresholds for low contrast lunar surface images
+ORBextractor.iniThFAST: 12
+ORBextractor.minThFAST: 4
+
+#--------------------------------------------------------------------------------------------
+# Viewer Parameters
+#--------------------------------------------------------------------------------------------
+Viewer.KeyFrameSize: 0.1
+Viewer.KeyFrameLineWidth: 1.0
+Viewer.GraphLineWidth: 0.9
+Viewer.PointSize: 2.0
+Viewer.CameraSize: 0.15
+Viewer.CameraLineWidth: 3.0
+Viewer.ViewpointX: 0.0
+Viewer.ViewpointY: -10.0
+Viewer.ViewpointZ: -30.0
+Viewer.ViewpointF: 500.0
diff --git a/Examples/Stereo/stereo_custom.cc b/Examples/Stereo/stereo_custom.cc
new file mode 100644
index 0000000..1d2ba36
--- /dev/null
+++ b/Examples/Stereo/stereo_custom.cc
@@ -0,0 +1,450 @@
+/**
+* Custom Stereo SLAM executable for user dataset
+* 
+* Dataset format:
+* - Left images in: left_dir/
+* - Right images in: right_dir/
+* - Timestamp file: left_dir/color_timestamp.txt (or right_dir/)
+* - GT file: automatically searched in parent directories (gt.txt)
+* - Timestamp file format:
+*   #timestamp[ns], filename
+*   1693274724086398208,1693274724086398208.png
+*
+* Usage:
+* ./stereo_custom path_to_vocabulary path_to_settings left_images_dir right_images_dir output_trajectory.txt
+*
+* The output trajectory is automatically aligned to GT coordinate system using:
+* 1. Coordinate axis transformation (ORB-SLAM3 camera -> GT world frame)
+* 2. Initial pose alignment from GT first pose
+*/
+
+#include <iostream>
+#include <algorithm>
+#include <fstream>
+#include <iomanip>
+#include <chrono>
+#include <sstream>
+#include <cmath>
+
+#include <opencv2/core/core.hpp>
+#include <opencv2/imgcodecs.hpp>
+
+#include "System.h"
+
+using namespace std;
+
+void LoadImages(const string &strPathLeft, const string &strPathRight,
+                vector<string> &vstrImageLeft, vector<string> &vstrImageRight, 
+                vector<double> &vTimeStamps);
+
+// Load initial pose from GT file (EuRoC IMU format)
+bool LoadInitialPoseFromGT(const string &strPathLeft, Eigen::Vector3f &t_init, Eigen::Quaternionf &q_init);
+
+// Find GT file in parent directories
+string FindGTFile(const string &strPathLeft);
+
+int main(int argc, char **argv)
+{
+    if(argc < 6)
+    {
+        cerr << endl << "Usage: ./stereo_custom path_to_vocabulary path_to_settings left_images_dir right_images_dir output_trajectory.txt" << endl;
+        return 1;
+    }
+
+    string vocPath = argv[1];
+    string settingsPath = argv[2];
+    string leftDir = argv[3];
+    string rightDir = argv[4];
+    string outputFile = argv[5];
+
+    // Load images
+    vector<string> vstrImageLeft;
+    vector<string> vstrImageRight;
+    vector<double> vTimeStamps;
+
+    LoadImages(leftDir, rightDir, vstrImageLeft, vstrImageRight, vTimeStamps);
+
+    const int nImages = vstrImageLeft.size();
+
+    if(nImages <= 0)
+    {
+        cerr << "ERROR: No images found!" << endl;
+        return 1;
+    }
+
+    cout << endl << "-------" << endl;
+    cout << "Found " << nImages << " images" << endl;
+    cout << "Left dir: " << leftDir << endl;
+    cout << "Right dir: " << rightDir << endl;
+
+    // Load initial pose from GT file for alignment
+    Eigen::Vector3f t_gt_init = Eigen::Vector3f::Zero();
+    Eigen::Quaternionf q_gt_init = Eigen::Quaternionf::Identity();
+    bool hasGTInit = LoadInitialPoseFromGT(leftDir, t_gt_init, q_gt_init);
+    
+    if(hasGTInit)
+    {
+        cout << "Loaded GT initial pose: t=(" << t_gt_init.transpose() << "), q=(" 
+             << q_gt_init.w() << ", " << q_gt_init.x() << ", " << q_gt_init.y() << ", " << q_gt_init.z() << ")" << endl;
+    }
+    else
+    {
+        cout << "Warning: GT file not found. Output will start from origin." << endl;
+    }
+
+    // Create SLAM system
+    ORB_SLAM3::System SLAM(vocPath, settingsPath, ORB_SLAM3::System::STEREO, true);
+    float imageScale = SLAM.GetImageScale();
+
+    // Open output file
+    ofstream outFile(outputFile);
+    if(!outFile.is_open())
+    {
+        cerr << "ERROR: Cannot open output file: " << outputFile << endl;
+        return 1;
+    }
+    outFile << fixed << setprecision(9);
+
+    // Vector for tracking time statistics
+    vector<float> vTimesTrack;
+    vTimesTrack.resize(nImages);
+
+    cout << endl << "-------" << endl;
+    cout << "Start processing sequence ..." << endl;
+    cout << "Images in the sequence: " << nImages << endl << endl;
+
+    double t_resize = 0.f;
+    double t_track = 0.f;
+
+    // Variables for trajectory alignment
+    // We'll capture the first valid SLAM pose and use it to transform all subsequent poses
+    // so that the SLAM trajectory starts from GT's initial pose
+    bool bFirstValidPose = false;
+    Sophus::SE3f T_slam_first;  // First valid SLAM pose (world-to-camera inverse = camera-in-world)
+    Sophus::SE3f T_gt_init;     // GT initial pose (camera-in-world from GT)
+    
+    if(hasGTInit)
+    {
+        // Build SE3 from GT initial pose
+        Eigen::Matrix3f R_gt = q_gt_init.toRotationMatrix();
+        T_gt_init = Sophus::SE3f(R_gt, t_gt_init);
+        cout << "GT initial transformation ready for alignment." << endl;
+    }
+
+    cv::Mat imLeft, imRight;
+    for(int ni = 0; ni < nImages; ni++)
+    {
+        // Read images
+        imLeft = cv::imread(vstrImageLeft[ni], cv::IMREAD_UNCHANGED);
+        imRight = cv::imread(vstrImageRight[ni], cv::IMREAD_UNCHANGED);
+        double tframe = vTimeStamps[ni];
+
+        if(imLeft.empty())
+        {
+            cerr << endl << "Failed to load image at: " << vstrImageLeft[ni] << endl;
+            continue;
+        }
+        if(imRight.empty())
+        {
+            cerr << endl << "Failed to load image at: " << vstrImageRight[ni] << endl;
+            continue;
+        }
+
+        // Resize if needed
+        if(imageScale != 1.f)
+        {
+            int width = imLeft.cols * imageScale;
+            int height = imLeft.rows * imageScale;
+            cv::resize(imLeft, imLeft, cv::Size(width, height));
+            cv::resize(imRight, imRight, cv::Size(width, height));
+        }
+
+        // Track
+        std::chrono::steady_clock::time_point t1 = std::chrono::steady_clock::now();
+        
+        Sophus::SE3f Tcw = SLAM.TrackStereo(imLeft, imRight, tframe);
+        
+        std::chrono::steady_clock::time_point t2 = std::chrono::steady_clock::now();
+
+        double ttrack = std::chrono::duration_cast<std::chrono::duration<double>>(t2 - t1).count();
+        vTimesTrack[ni] = ttrack;
+
+        // Save trajectory in real-time (TUM format: timestamp tx ty tz qx qy qz qw)
+        if(!Tcw.matrix().isZero(0))
+        {
+            // Convert camera pose to world pose (camera-in-world)
+            Sophus::SE3f Twc = Tcw.inverse();
+            
+            // Capture first valid SLAM pose for alignment
+            if(!bFirstValidPose)
+            {
+                T_slam_first = Twc;
+                bFirstValidPose = true;
+                cout << "First valid SLAM pose captured at frame " << ni << endl;
+            }
+            
+            // Get relative pose from first frame
+            // T_rel = T_slam_first^(-1) * T_slam_current (relative movement in camera frame)
+            Sophus::SE3f T_rel = T_slam_first.inverse() * Twc;
+            
+            // ============================================================
+            // COORDINATE SYSTEM TRANSFORMATION
+            // ============================================================
+            // 
+            // Camera is mounted on rover with:
+            //   - Tilt: camera_tilt_deg° (tilted down from horizontal)
+            //   - Yaw: 90° (rotated sideways)
+            //
+            // This rotation matrix accounts for:
+            //   1. Camera coordinate system (X-right, Y-down, Z-forward)
+            //   2. World coordinate system (X-forward, Y-right, Z-up)
+            //   3. Camera mounting tilt angle
+            // ============================================================
+            
+            // Camera tilt angle (degrees) - adjust this value based on camera mounting
+            const float camera_tilt_deg = 70.077f;
+            const float camera_tilt_rad = camera_tilt_deg * M_PI / 180.0f;
+            const float cos_tilt = std::cos(camera_tilt_rad);
+            const float sin_tilt = std::sin(camera_tilt_rad);
+            
+            Eigen::Matrix3f R_cam_to_world;
+            R_cam_to_world << 0.0f, -cos_tilt, sin_tilt,
+                              1.0f,  0.0f,     0.0f,
+                              0.0f,  sin_tilt, cos_tilt;
+            
+            // Apply coordinate transformation to relative pose
+            Eigen::Vector3f t_rel_cam = T_rel.translation();
+            Eigen::Matrix3f R_rel_cam = T_rel.rotationMatrix();
+            
+            // Transform translation and rotation to world coordinate system
+            Eigen::Vector3f t_rel_world = R_cam_to_world * t_rel_cam;
+            Eigen::Matrix3f R_rel_world = R_cam_to_world * R_rel_cam * R_cam_to_world.transpose();
+            
+            // Re-orthogonalize the rotation matrix to avoid Sophus numerical precision errors
+            // Use SVD to project back onto SO(3): R = U * V^T where R = U * S * V^T
+            Eigen::JacobiSVD<Eigen::Matrix3f> svd(R_rel_world, Eigen::ComputeFullU | Eigen::ComputeFullV);
+            R_rel_world = svd.matrixU() * svd.matrixV().transpose();
+            // Ensure proper rotation (det = +1, not reflection)
+            if (R_rel_world.determinant() < 0) {
+                R_rel_world = -R_rel_world;
+            }
+            
+            Sophus::SE3f T_rel_world(R_rel_world, t_rel_world);
+            
+            // Apply GT initial pose: T_final = T_gt_init * T_rel_world
+            Sophus::SE3f Twc_aligned;
+            if(hasGTInit)
+            {
+                Twc_aligned = T_gt_init * T_rel_world;
+            }
+            else
+            {
+                // No GT available, just apply coordinate transformation
+                Twc_aligned = T_rel_world;
+            }
+            
+            Eigen::Vector3f twc = Twc_aligned.translation();
+            Eigen::Quaternionf q = Twc_aligned.unit_quaternion();
+
+            // Output aligned trajectory
+            outFile << tframe << " " 
+                    << twc(0) << " " << twc(1) << " " << twc(2) << " "
+                    << q.x() << " " << q.y() << " " << q.z() << " " << q.w() << endl;
+            outFile.flush();
+        }
+
+        // Wait to maintain real-time pace
+        double T = 0;
+        if(ni < nImages - 1)
+            T = vTimeStamps[ni + 1] - tframe;
+        else if(ni > 0)
+            T = tframe - vTimeStamps[ni - 1];
+
+        if(ttrack < T)
+            usleep((T - ttrack) * 1e6);
+
+        // Print progress
+        if(ni % 100 == 0)
+        {
+            cout << "Processing frame " << ni << "/" << nImages << endl;
+        }
+    }
+
+    // Stop all threads
+    SLAM.Shutdown();
+
+    outFile.close();
+
+    // Tracking time statistics
+    sort(vTimesTrack.begin(), vTimesTrack.end());
+    float totaltime = 0;
+    for(int ni = 0; ni < nImages; ni++)
+    {
+        totaltime += vTimesTrack[ni];
+    }
+    cout << "-------" << endl << endl;
+    cout << "median tracking time: " << vTimesTrack[nImages / 2] << endl;
+    cout << "mean tracking time: " << totaltime / nImages << endl;
+
+    // Save final trajectories
+    SLAM.SaveTrajectoryTUM("CameraTrajectory_final.txt");
+    SLAM.SaveKeyFrameTrajectoryTUM("KeyFrameTrajectory_final.txt");
+
+    cout << endl << "Trajectory saved to: " << outputFile << endl;
+
+    return 0;
+}
+
+void LoadImages(const string &strPathLeft, const string &strPathRight,
+                vector<string> &vstrImageLeft, vector<string> &vstrImageRight, 
+                vector<double> &vTimeStamps)
+{
+    // Try to find timestamp file in left directory
+    string timestampFile = strPathLeft + "/color_timestamp.txt";
+    ifstream fTimes(timestampFile);
+    
+    // If not found, try parent directory of left directory
+    if(!fTimes.is_open())
+    {
+        timestampFile = strPathLeft + "/../color_timestamp.txt";
+        fTimes.open(timestampFile);
+    }
+
+    if(!fTimes.is_open())
+    {
+        // Try right directory
+        timestampFile = strPathRight + "/color_timestamp.txt";
+        fTimes.open(timestampFile);
+    }
+
+    // If not found, try parent directory of right directory
+    if(!fTimes.is_open())
+    {
+        timestampFile = strPathRight + "/../color_timestamp.txt";
+        fTimes.open(timestampFile);
+    }
+    
+    if(!fTimes.is_open())
+    {
+        cerr << "ERROR: Cannot find color_timestamp.txt in " << strPathLeft << " or parent, or " << strPathRight << " or parent" << endl;
+        return;
+    }
+
+    cout << "Loading timestamps from: " << timestampFile << endl;
+
+    string line;
+    while(getline(fTimes, line))
+    {
+        // Skip empty lines and comments
+        if(line.empty() || line[0] == '#')
+            continue;
+
+        // Parse CSV: timestamp,filename
+        stringstream ss(line);
+        string timestampStr, filename;
+        
+        if(getline(ss, timestampStr, ',') && getline(ss, filename, ','))
+        {
+            // Remove any whitespace
+            timestampStr.erase(remove_if(timestampStr.begin(), timestampStr.end(), ::isspace), timestampStr.end());
+            filename.erase(remove_if(filename.begin(), filename.end(), ::isspace), filename.end());
+            
+            // Convert timestamp (nanoseconds to seconds)
+            double timestamp = stod(timestampStr) / 1e9;
+            vTimeStamps.push_back(timestamp);
+            
+            // Build full paths
+            vstrImageLeft.push_back(strPathLeft + "/" + filename);
+            vstrImageRight.push_back(strPathRight + "/" + filename);
+        }
+    }
+    fTimes.close();
+
+    cout << "Loaded " << vTimeStamps.size() << " timestamps" << endl;
+}
+
+string FindGTFile(const string &strPathLeft)
+{
+    // Try different possible locations for gt.txt
+    vector<string> candidates = {
+        strPathLeft + "/gt.txt",
+        strPathLeft + "/../gt.txt",
+        strPathLeft + "/../../gt.txt",
+        strPathLeft + "/../../../gt.txt"
+    };
+    
+    for(const auto& path : candidates)
+    {
+        ifstream f(path);
+        if(f.is_open())
+        {
+            f.close();
+            return path;
+        }
+    }
+    return "";
+}
+
+bool LoadInitialPoseFromGT(const string &strPathLeft, Eigen::Vector3f &t_init, Eigen::Quaternionf &q_init)
+{
+    string gtFile = FindGTFile(strPathLeft);
+    if(gtFile.empty())
+    {
+        return false;
+    }
+    
+    cout << "Loading GT initial pose from: " << gtFile << endl;
+    
+    ifstream fGT(gtFile);
+    if(!fGT.is_open())
+    {
+        return false;
+    }
+    
+    string line;
+    while(getline(fGT, line))
+    {
+        // Skip empty lines and comments
+        if(line.empty() || line[0] == '#')
+            continue;
+        
+        // Parse EuRoC IMU format: timestamp,x,y,z,qw,qx,qy,qz,...
+        stringstream ss(line);
+        string token;
+        vector<double> values;
+        
+        while(getline(ss, token, ','))
+        {
+            // Remove whitespace
+            token.erase(remove_if(token.begin(), token.end(), ::isspace), token.end());
+            if(!token.empty())
+            {
+                try {
+                    values.push_back(stod(token));
+                } catch(...) {
+                    continue;
+                }
+            }
+        }
+        
+        // Need at least timestamp + position (3) + quaternion (4) = 8 values
+        if(values.size() >= 8)
+        {
+            // EuRoC format: timestamp, p_x, p_y, p_z, q_w, q_x, q_y, q_z, ...
+            t_init(0) = values[1];  // x
+            t_init(1) = values[2];  // y
+            t_init(2) = values[3];  // z
+            q_init.w() = values[4]; // qw
+            q_init.x() = values[5]; // qx
+            q_init.y() = values[6]; // qy
+            q_init.z() = values[7]; // qz
+            q_init.normalize();
+            
+            fGT.close();
+            return true;
+        }
+    }
+    
+    fGT.close();
+    return false;
+}
diff --git a/Examples/Stereo/stereo_custom_dataset.cc b/Examples/Stereo/stereo_custom_dataset.cc
new file mode 100644
index 0000000..6eb43ae
--- /dev/null
+++ b/Examples/Stereo/stereo_custom_dataset.cc
@@ -0,0 +1,579 @@
+/**
+* Custom Stereo SLAM executable for new dataset format
+* 
+* Dataset format (frame_index.txt):
+* timestamp: left_img, right_img, depth_npy, tf_file
+* 
+* Example:
+* 1353_400070585: -, -, -, tf_1353_400070585.txt
+* 1353_483403922: left_1353_483403922.png, right_1353_483403922.png, depth_1353_483403922.npy, -
+*
+* TF file format (ROS2 TFMessage):
+* tf2_msgs.msg.TFMessage(transforms=[geometry_msgs.msg.TransformStamped(..., 
+*   transform=geometry_msgs.msg.Transform(
+*     translation=geometry_msgs.msg.Vector3(x=..., y=..., z=...),
+*     rotation=geometry_msgs.msg.Quaternion(x=..., y=..., z=..., w=...)))])
+*
+* Features:
+* - Parses frame_index.txt for image pairs and tf files
+* - Loads stereo pairs based on left image timestamp
+* - Finds closest right image when timestamps don't match
+* - Parses GT trajectory from tf files
+*
+* Usage:
+* ./stereo_custom_dataset path_to_vocabulary path_to_settings path_to_dataset output_trajectory.txt
+*/
+
+#include <iostream>
+#include <algorithm>
+#include <fstream>
+#include <iomanip>
+#include <chrono>
+#include <sstream>
+#include <cmath>
+#include <map>
+#include <regex>
+
+#include <opencv2/core/core.hpp>
+#include <opencv2/imgcodecs.hpp>
+
+#include "System.h"
+
+using namespace std;
+
+// Structure to hold frame data
+struct FrameData {
+    double timestamp;           // Timestamp in seconds
+    string timestamp_str;       // Original timestamp string (e.g., "1353_483403922")
+    string left_img;            // Left image filename (empty if none)
+    string right_img;           // Right image filename (empty if none)
+    string depth_file;          // Depth file (not used for stereo)
+    string tf_file;             // TF file for GT pose
+};
+
+// Structure to hold GT pose
+struct GTPose {
+    double timestamp;
+    double x, y, z;
+    double qx, qy, qz, qw;
+};
+
+// Function declarations
+void LoadFrameIndex(const string &datasetPath, 
+                    vector<FrameData> &frames,
+                    vector<GTPose> &gtPoses);
+
+bool ParseTFFile(const string &tfFilePath, GTPose &pose);
+
+double TimestampStrToSeconds(const string &ts_str);
+
+string Trim(const string &str);
+
+int main(int argc, char **argv)
+{
+    if(argc < 5)
+    {
+        cerr << endl << "Usage: ./stereo_custom_dataset path_to_vocabulary path_to_settings path_to_dataset output_trajectory.txt" << endl;
+        cerr << endl << "Example: ./stereo_custom_dataset Vocabulary/ORBvoc.txt Examples/Stereo/lunar.yaml ./dataset estimated_trajectory.txt" << endl;
+        return 1;
+    }
+
+    string vocPath = argv[1];
+    string settingsPath = argv[2];
+    string datasetPath = argv[3];
+    string outputFile = argv[4];
+
+    // Ensure dataset path ends without slash
+    if(!datasetPath.empty() && datasetPath.back() == '/')
+        datasetPath.pop_back();
+
+    // Load frame index and GT poses
+    vector<FrameData> frames;
+    vector<GTPose> gtPoses;
+    LoadFrameIndex(datasetPath, frames, gtPoses);
+
+    // Filter frames that have left images (for SLAM processing)
+    vector<FrameData> validFrames;
+    map<int, string> rightImages;  // Map sec -> right image for matching
+    
+    for(const auto &frame : frames)
+    {
+        if(!frame.right_img.empty())
+        {
+            // Extract second part from timestamp for right image lookup
+            size_t pos = frame.timestamp_str.find('_');
+            if(pos != string::npos)
+            {
+                int sec = stoi(frame.timestamp_str.substr(0, pos));
+                rightImages[sec] = frame.right_img;
+            }
+        }
+        
+        if(!frame.left_img.empty())
+        {
+            validFrames.push_back(frame);
+        }
+    }
+
+    // For frames with left images but no right images, find closest right image
+    for(auto &frame : validFrames)
+    {
+        if(frame.right_img.empty())
+        {
+            // Extract second from timestamp
+            size_t pos = frame.timestamp_str.find('_');
+            if(pos != string::npos)
+            {
+                int sec = stoi(frame.timestamp_str.substr(0, pos));
+                if(rightImages.count(sec))
+                {
+                    frame.right_img = rightImages[sec];
+                    cout << "Matched right image for " << frame.timestamp_str << ": " << frame.right_img << endl;
+                }
+            }
+        }
+    }
+
+    // Remove frames that still don't have right images
+    vector<FrameData> stereoFrames;
+    for(const auto &frame : validFrames)
+    {
+        if(!frame.right_img.empty())
+        {
+            stereoFrames.push_back(frame);
+        }
+        else
+        {
+            cerr << "Warning: No right image found for " << frame.timestamp_str << ", skipping frame" << endl;
+        }
+    }
+
+    const int nImages = stereoFrames.size();
+
+    if(nImages <= 0)
+    {
+        cerr << "ERROR: No valid stereo pairs found!" << endl;
+        return 1;
+    }
+
+    cout << endl << "-------" << endl;
+    cout << "Dataset path: " << datasetPath << endl;
+    cout << "Found " << nImages << " stereo pairs" << endl;
+    cout << "Found " << gtPoses.size() << " GT poses" << endl;
+
+    // Save GT trajectory to file
+    string gtOutputFile = datasetPath + "/gt_trajectory.txt";
+    ofstream gtOut(gtOutputFile);
+    if(gtOut.is_open())
+    {
+        gtOut << fixed << setprecision(9);
+        gtOut << "# GT Trajectory from TF files" << endl;
+        gtOut << "# timestamp tx ty tz qx qy qz qw" << endl;
+        for(const auto &pose : gtPoses)
+        {
+            gtOut << pose.timestamp << " "
+                  << pose.x << " " << pose.y << " " << pose.z << " "
+                  << pose.qx << " " << pose.qy << " " << pose.qz << " " << pose.qw << endl;
+        }
+        gtOut.close();
+        cout << "GT trajectory saved to: " << gtOutputFile << endl;
+    }
+
+    // Get GT initial pose for alignment
+    Eigen::Vector3f t_gt_init = Eigen::Vector3f::Zero();
+    Eigen::Quaternionf q_gt_init = Eigen::Quaternionf::Identity();
+    bool hasGTInit = false;
+    
+    if(!gtPoses.empty())
+    {
+        const auto &firstGT = gtPoses[0];
+        t_gt_init(0) = firstGT.x;
+        t_gt_init(1) = firstGT.y;
+        t_gt_init(2) = firstGT.z;
+        q_gt_init.x() = firstGT.qx;
+        q_gt_init.y() = firstGT.qy;
+        q_gt_init.z() = firstGT.qz;
+        q_gt_init.w() = firstGT.qw;
+        q_gt_init.normalize();
+        hasGTInit = true;
+        
+        cout << "GT initial pose: t=(" << t_gt_init.transpose() << "), q=(" 
+             << q_gt_init.w() << ", " << q_gt_init.x() << ", " << q_gt_init.y() << ", " << q_gt_init.z() << ")" << endl;
+    }
+
+    // Create SLAM system
+    ORB_SLAM3::System SLAM(vocPath, settingsPath, ORB_SLAM3::System::STEREO, true);
+    float imageScale = SLAM.GetImageScale();
+
+    // Open output file
+    ofstream outFile(outputFile);
+    if(!outFile.is_open())
+    {
+        cerr << "ERROR: Cannot open output file: " << outputFile << endl;
+        return 1;
+    }
+    outFile << fixed << setprecision(9);
+    outFile << "# Estimated Trajectory" << endl;
+    outFile << "# timestamp tx ty tz qx qy qz qw" << endl;
+
+    // Vector for tracking time statistics
+    vector<float> vTimesTrack;
+    vTimesTrack.resize(nImages);
+
+    cout << endl << "-------" << endl;
+    cout << "Start processing sequence ..." << endl;
+    cout << "Images in the sequence: " << nImages << endl << endl;
+
+    // Variables for trajectory alignment
+    bool bFirstValidPose = false;
+    Sophus::SE3f T_slam_first;
+    Sophus::SE3f T_gt_init_se3;
+    
+    if(hasGTInit)
+    {
+        Eigen::Matrix3f R_gt = q_gt_init.toRotationMatrix();
+        T_gt_init_se3 = Sophus::SE3f(R_gt, t_gt_init);
+    }
+
+    string dataDir = datasetPath + "/data";
+    cv::Mat imLeft, imRight;
+    
+    for(int ni = 0; ni < nImages; ni++)
+    {
+        const FrameData &frame = stereoFrames[ni];
+        
+        // Build full paths
+        string leftPath = dataDir + "/" + frame.left_img;
+        string rightPath = dataDir + "/" + frame.right_img;
+        
+        // Read images
+        imLeft = cv::imread(leftPath, cv::IMREAD_UNCHANGED);
+        imRight = cv::imread(rightPath, cv::IMREAD_UNCHANGED);
+        double tframe = frame.timestamp;
+
+        if(imLeft.empty())
+        {
+            cerr << endl << "Failed to load left image at: " << leftPath << endl;
+            continue;
+        }
+        if(imRight.empty())
+        {
+            cerr << endl << "Failed to load right image at: " << rightPath << endl;
+            continue;
+        }
+
+        // Resize if needed
+        if(imageScale != 1.f)
+        {
+            int width = imLeft.cols * imageScale;
+            int height = imLeft.rows * imageScale;
+            cv::resize(imLeft, imLeft, cv::Size(width, height));
+            cv::resize(imRight, imRight, cv::Size(width, height));
+        }
+
+        // Track
+        std::chrono::steady_clock::time_point t1 = std::chrono::steady_clock::now();
+        
+        Sophus::SE3f Tcw = SLAM.TrackStereo(imLeft, imRight, tframe);
+        
+        std::chrono::steady_clock::time_point t2 = std::chrono::steady_clock::now();
+
+        double ttrack = std::chrono::duration_cast<std::chrono::duration<double>>(t2 - t1).count();
+        vTimesTrack[ni] = ttrack;
+
+        // Save trajectory in real-time
+        if(!Tcw.matrix().isZero(0))
+        {
+            Sophus::SE3f Twc = Tcw.inverse();
+            
+            if(!bFirstValidPose)
+            {
+                T_slam_first = Twc;
+                bFirstValidPose = true;
+                cout << "First valid SLAM pose captured at frame " << ni << endl;
+            }
+            
+            Sophus::SE3f T_rel = T_slam_first.inverse() * Twc;
+            
+            // ============================================================
+            // COORDINATE SYSTEM TRANSFORMATION
+            // ============================================================
+            // Camera coordinate system: Z(forward), Y(up), X(right)
+            // World/Body coordinate system: Z-UP (X-forward, Y-left, Z-up)
+            // Camera tilt: 20 degrees (tilted down from horizontal)
+            //
+            // The rotation matrix R transforms a point from camera frame to body frame:
+            //   p_body = R * p_camera
+            //
+            // Camera axes in body frame (when camera is level, no tilt):
+            //   Camera_X (right)   -> Body_Y (but negative, so -Y = left direction is right)
+            //   Camera_Y (up)      -> Body_Z (up)
+            //   Camera_Z (forward) -> Body_X (forward)
+            //
+            // Camera tilt: camera is pitched down by 20 degrees
+            //   This means camera's Z axis points 20 deg below horizontal
+            //   To correct: rotate the result UP by 20 degrees (negative pitch)
+            // ============================================================
+            
+            const float camera_tilt_deg = 20.0f;  // Camera tilted 20 degrees down
+            const float camera_tilt_rad = camera_tilt_deg * M_PI / 180.0f;
+            const float cos_tilt = std::cos(camera_tilt_rad);
+            const float sin_tilt = std::sin(camera_tilt_rad);
+            
+            // Camera to body frame transformation (without tilt)
+            // Camera: X(right), Y(up), Z(forward) -> Body: X(forward), Y(left), Z(up)
+            // This matrix's columns are camera axes expressed in body frame
+            //   Column 0 (Camera_X in body): Camera_X(right) = -Body_Y -> [0, -1, 0]
+            //   Column 1 (Camera_Y in body): Camera_Y(up) = Body_Z -> [0, 0, 1]
+            //   Column 2 (Camera_Z in body): Camera_Z(forward) = Body_X -> [1, 0, 0]
+            Eigen::Matrix3f R_cam_to_body;
+            R_cam_to_body << 0.0f, 0.0f, 1.0f,   // Body_X component from [Cam_X, Cam_Y, Cam_Z]
+                            -1.0f, 0.0f, 0.0f,   // Body_Y component from [Cam_X, Cam_Y, Cam_Z]
+                             0.0f, 1.0f, 0.0f;   // Body_Z component from [Cam_X, Cam_Y, Cam_Z]
+            
+            // Tilt correction: rotate around Body_Y axis (pitch UP to compensate for camera looking down)
+            // Rotation around Y-axis by +tilt_angle (counterclockwise when viewed from +Y)
+            // This rotates X toward Z (forward toward up)
+            Eigen::Matrix3f R_tilt_correction;
+            R_tilt_correction << cos_tilt,  0.0f, -sin_tilt,
+                                 0.0f,      1.0f,  0.0f,
+                                 sin_tilt,  0.0f,  cos_tilt;
+            
+            // Combined transformation: first camera-to-body, then tilt correction
+            Eigen::Matrix3f R_cam_to_world = R_tilt_correction * R_cam_to_body;
+            
+            Eigen::Vector3f t_rel_cam = T_rel.translation();
+            Eigen::Matrix3f R_rel_cam = T_rel.rotationMatrix();
+            
+            // Transform translation and rotation to world coordinate system
+            Eigen::Vector3f t_rel_world = R_cam_to_world * t_rel_cam;
+            Eigen::Matrix3f R_rel_world = R_cam_to_world * R_rel_cam * R_cam_to_world.transpose();
+            
+            // Debug output for first few frames
+            if(ni < 5) {
+                cout << "Frame " << ni << " Camera translation: " << t_rel_cam.transpose() << endl;
+                cout << "Frame " << ni << " World translation: " << t_rel_world.transpose() << endl;
+            }
+            
+            // Re-orthogonalize rotation matrix
+            Eigen::JacobiSVD<Eigen::Matrix3f> svd(R_rel_world, Eigen::ComputeFullU | Eigen::ComputeFullV);
+            R_rel_world = svd.matrixU() * svd.matrixV().transpose();
+            if (R_rel_world.determinant() < 0) {
+                R_rel_world = -R_rel_world;
+            }
+            
+            Sophus::SE3f T_rel_world(R_rel_world, t_rel_world);
+            
+            Sophus::SE3f Twc_aligned;
+            if(hasGTInit)
+            {
+                Twc_aligned = T_gt_init_se3 * T_rel_world;
+            }
+            else
+            {
+                Twc_aligned = T_rel_world;
+            }
+            
+            Eigen::Vector3f twc = Twc_aligned.translation();
+            Eigen::Quaternionf q = Twc_aligned.unit_quaternion();
+
+            outFile << tframe << " " 
+                    << twc(0) << " " << twc(1) << " " << twc(2) << " "
+                    << q.x() << " " << q.y() << " " << q.z() << " " << q.w() << endl;
+            outFile.flush();
+        }
+
+        // Wait to maintain real-time pace
+        double T = 0;
+        if(ni < nImages - 1)
+            T = stereoFrames[ni + 1].timestamp - tframe;
+        else if(ni > 0)
+            T = tframe - stereoFrames[ni - 1].timestamp;
+
+        if(ttrack < T)
+            usleep((T - ttrack) * 1e6);
+
+        // Print progress
+        if(ni % 10 == 0)
+        {
+            cout << "Processing frame " << ni << "/" << nImages 
+                 << " (" << frame.left_img << ")" << endl;
+        }
+    }
+
+    // Stop all threads
+    SLAM.Shutdown();
+
+    outFile.close();
+
+    // Tracking time statistics
+    sort(vTimesTrack.begin(), vTimesTrack.end());
+    float totaltime = 0;
+    for(int ni = 0; ni < nImages; ni++)
+    {
+        totaltime += vTimesTrack[ni];
+    }
+    cout << "-------" << endl << endl;
+    cout << "median tracking time: " << vTimesTrack[nImages / 2] << endl;
+    cout << "mean tracking time: " << totaltime / nImages << endl;
+
+    // Save final trajectories
+    SLAM.SaveTrajectoryTUM("CameraTrajectory_final.txt");
+    SLAM.SaveKeyFrameTrajectoryTUM("KeyFrameTrajectory_final.txt");
+
+    cout << endl << "Estimated trajectory saved to: " << outputFile << endl;
+    cout << "GT trajectory saved to: " << gtOutputFile << endl;
+
+    return 0;
+}
+
+string Trim(const string &str)
+{
+    size_t first = str.find_first_not_of(" \t\n\r");
+    if(first == string::npos)
+        return "";
+    size_t last = str.find_last_not_of(" \t\n\r");
+    return str.substr(first, (last - first + 1));
+}
+
+double TimestampStrToSeconds(const string &ts_str)
+{
+    // Format: "1353_483403922" -> sec=1353, nanosec=483403922
+    size_t pos = ts_str.find('_');
+    if(pos == string::npos)
+    {
+        cerr << "Invalid timestamp format: " << ts_str << endl;
+        return 0.0;
+    }
+    
+    long long sec = stoll(ts_str.substr(0, pos));
+    long long nanosec = stoll(ts_str.substr(pos + 1));
+    
+    return static_cast<double>(sec) + static_cast<double>(nanosec) / 1e9;
+}
+
+void LoadFrameIndex(const string &datasetPath, 
+                    vector<FrameData> &frames,
+                    vector<GTPose> &gtPoses)
+{
+    string indexFile = datasetPath + "/frame_index.txt";
+    ifstream fin(indexFile);
+    
+    if(!fin.is_open())
+    {
+        cerr << "ERROR: Cannot open frame_index.txt at: " << indexFile << endl;
+        return;
+    }
+    
+    cout << "Loading frame index from: " << indexFile << endl;
+    
+    string dataDir = datasetPath + "/data";
+    string line;
+    
+    while(getline(fin, line))
+    {
+        line = Trim(line);
+        if(line.empty() || line[0] == '#')
+            continue;
+        
+        // Parse format: "timestamp: left, right, depth, tf"
+        size_t colonPos = line.find(':');
+        if(colonPos == string::npos)
+            continue;
+        
+        string timestamp_str = Trim(line.substr(0, colonPos));
+        string rest = line.substr(colonPos + 1);
+        
+        // Split by comma
+        stringstream ss(rest);
+        string token;
+        vector<string> parts;
+        
+        while(getline(ss, token, ','))
+        {
+            parts.push_back(Trim(token));
+        }
+        
+        if(parts.size() < 4)
+        {
+            cerr << "Invalid line format: " << line << endl;
+            continue;
+        }
+        
+        FrameData frame;
+        frame.timestamp_str = timestamp_str;
+        frame.timestamp = TimestampStrToSeconds(timestamp_str);
+        frame.left_img = (parts[0] != "-") ? parts[0] : "";
+        frame.right_img = (parts[1] != "-") ? parts[1] : "";
+        frame.depth_file = (parts[2] != "-") ? parts[2] : "";
+        frame.tf_file = (parts[3] != "-") ? parts[3] : "";
+        
+        frames.push_back(frame);
+        
+        // If this frame has a TF file, parse GT pose
+        if(!frame.tf_file.empty())
+        {
+            string tfPath = dataDir + "/" + frame.tf_file;
+            GTPose pose;
+            pose.timestamp = frame.timestamp;
+            
+            if(ParseTFFile(tfPath, pose))
+            {
+                gtPoses.push_back(pose);
+            }
+        }
+    }
+    
+    fin.close();
+    
+    cout << "Loaded " << frames.size() << " frame entries" << endl;
+}
+
+bool ParseTFFile(const string &tfFilePath, GTPose &pose)
+{
+    ifstream fin(tfFilePath);
+    if(!fin.is_open())
+    {
+        cerr << "Warning: Cannot open TF file: " << tfFilePath << endl;
+        return false;
+    }
+    
+    // Read entire file content
+    stringstream buffer;
+    buffer << fin.rdbuf();
+    string content = buffer.str();
+    fin.close();
+    
+    // Parse translation using regex
+    // Looking for: translation=geometry_msgs.msg.Vector3(x=..., y=..., z=...)
+    regex translationRegex(R"(translation=geometry_msgs\.msg\.Vector3\(x=([-\d.e]+),\s*y=([-\d.e]+),\s*z=([-\d.e]+)\))");
+    smatch translationMatch;
+    
+    if(!regex_search(content, translationMatch, translationRegex))
+    {
+        cerr << "Warning: Cannot parse translation from: " << tfFilePath << endl;
+        return false;
+    }
+    
+    pose.x = stod(translationMatch[1].str());
+    pose.y = stod(translationMatch[2].str());
+    pose.z = stod(translationMatch[3].str());
+    
+    // Parse rotation using regex
+    // Looking for: rotation=geometry_msgs.msg.Quaternion(x=..., y=..., z=..., w=...)
+    regex rotationRegex(R"(rotation=geometry_msgs\.msg\.Quaternion\(x=([-\d.e]+),\s*y=([-\d.e]+),\s*z=([-\d.e]+),\s*w=([-\d.e]+)\))");
+    smatch rotationMatch;
+    
+    if(!regex_search(content, rotationMatch, rotationRegex))
+    {
+        cerr << "Warning: Cannot parse rotation from: " << tfFilePath << endl;
+        return false;
+    }
+    
+    pose.qx = stod(rotationMatch[1].str());
+    pose.qy = stod(rotationMatch[2].str());
+    pose.qz = stod(rotationMatch[3].str());
+    pose.qw = stod(rotationMatch[4].str());
+    
+    return true;
+}
diff --git a/Examples/Stereo/stereo_euroc_realtime.cc b/Examples/Stereo/stereo_euroc_realtime.cc
new file mode 100644
index 0000000..6d6763b
--- /dev/null
+++ b/Examples/Stereo/stereo_euroc_realtime.cc
@@ -0,0 +1,210 @@
+/**
+* ORB-SLAM3 실시간 궤적 저장 버전
+* 
+* 이 예제는 매 프레임마다 추정된 포즈를 파일에 저장하여
+* 외부 스크립트에서 실시간 시각화가 가능하도록 합니다.
+*
+* 빌드:
+*   이 파일을 Examples/Stereo/ 폴더에 복사한 후
+*   CMakeLists.txt에 추가하고 빌드하거나,
+*   기존 stereo_euroc.cc를 백업 후 교체
+* 
+* 사용법:
+*   ./stereo_euroc_realtime path_to_vocabulary path_to_settings path_to_sequence path_to_times output_trajectory
+*/
+
+#include<iostream>
+#include<algorithm>
+#include<fstream>
+#include<iomanip>
+#include<chrono>
+
+#include<opencv2/core/core.hpp>
+
+#include<System.h>
+
+using namespace std;
+
+void LoadImages(const string &strPathLeft, const string &strPathRight, const string &strPathTimes,
+                vector<string> &vstrImageLeft, vector<string> &vstrImageRight, vector<double> &vTimeStamps);
+
+// 실시간 궤적 저장 함수
+void SavePoseRealtime(ofstream &f, double timestamp, const Sophus::SE3f &pose)
+{
+    if(!pose.matrix().isZero(0))
+    {
+        Eigen::Vector3f t = pose.translation();
+        Eigen::Quaternionf q = pose.unit_quaternion();
+        
+        f << fixed << setprecision(9) 
+          << timestamp << " "
+          << t(0) << " " << t(1) << " " << t(2) << " "
+          << q.x() << " " << q.y() << " " << q.z() << " " << q.w() << endl;
+        f.flush();  // 즉시 파일에 기록
+    }
+}
+
+int main(int argc, char **argv)
+{  
+    if(argc < 6)
+    {
+        cerr << endl << "Usage: ./stereo_euroc_realtime path_to_vocabulary path_to_settings path_to_sequence path_to_times output_trajectory" << endl;
+        return 1;
+    }
+
+    string pathVoc = string(argv[1]);
+    string pathSettings = string(argv[2]);
+    string pathSeq = string(argv[3]);
+    string pathTimes = string(argv[4]);
+    string outputFile = string(argv[5]);
+
+    // 이미지 로드
+    vector<string> vstrImageLeft;
+    vector<string> vstrImageRight;
+    vector<double> vTimestampsCam;
+
+    string pathCam0 = pathSeq + "/mav0/cam0/data";
+    string pathCam1 = pathSeq + "/mav0/cam1/data";
+
+    cout << "Loading images..." << endl;
+    LoadImages(pathCam0, pathCam1, pathTimes, vstrImageLeft, vstrImageRight, vTimestampsCam);
+    cout << "Loaded " << vstrImageLeft.size() << " images" << endl;
+
+    int nImages = vstrImageLeft.size();
+    if(nImages <= 0)
+    {
+        cerr << "ERROR: No images found!" << endl;
+        return 1;
+    }
+
+    // 실시간 궤적 저장 파일 열기
+    ofstream fTrajectory;
+    fTrajectory.open(outputFile.c_str());
+    fTrajectory << fixed;
+    
+    if(!fTrajectory.is_open())
+    {
+        cerr << "ERROR: Cannot open output file: " << outputFile << endl;
+        return 1;
+    }
+    
+    cout << "Real-time trajectory will be saved to: " << outputFile << endl;
+
+    // SLAM 시스템 생성
+    ORB_SLAM3::System SLAM(pathVoc, pathSettings, ORB_SLAM3::System::STEREO, true);
+
+    // 트래킹 시간 통계
+    vector<float> vTimesTrack;
+    vTimesTrack.resize(nImages);
+
+    cout << endl << "-------" << endl;
+    cout << "Start processing sequence ..." << endl;
+    cout << "Images in the sequence: " << nImages << endl << endl;
+
+    cv::Mat imLeft, imRight;
+    
+    for(int ni=0; ni<nImages; ni++)
+    {
+        // 이미지 로드
+        imLeft = cv::imread(vstrImageLeft[ni], cv::IMREAD_UNCHANGED);
+        imRight = cv::imread(vstrImageRight[ni], cv::IMREAD_UNCHANGED);
+        double tframe = vTimestampsCam[ni];
+
+        if(imLeft.empty())
+        {
+            cerr << endl << "Failed to load image at: " << vstrImageLeft[ni] << endl;
+            continue;
+        }
+
+        if(imRight.empty())
+        {
+            cerr << endl << "Failed to load image at: " << vstrImageRight[ni] << endl;
+            continue;
+        }
+
+        // 시간 측정 시작
+        std::chrono::steady_clock::time_point t1 = std::chrono::steady_clock::now();
+
+        // SLAM 트래킹 - 포즈 반환
+        Sophus::SE3f Tcw = SLAM.TrackStereo(imLeft, imRight, tframe, vector<ORB_SLAM3::IMU::Point>(), vstrImageLeft[ni]);
+
+        // 시간 측정 종료
+        std::chrono::steady_clock::time_point t2 = std::chrono::steady_clock::now();
+        double ttrack = std::chrono::duration_cast<std::chrono::duration<double>>(t2 - t1).count();
+        vTimesTrack[ni] = ttrack;
+
+        // 실시간 궤적 저장 (World 좌표계 기준)
+        // Tcw는 Camera-to-World 변환의 역이므로, World-to-Camera
+        // 카메라 위치를 World 좌표계에서 표현하려면 역변환 필요
+        if(!Tcw.matrix().isZero(0))
+        {
+            Sophus::SE3f Twc = Tcw.inverse();  // World 좌표계에서의 카메라 포즈
+            SavePoseRealtime(fTrajectory, tframe * 1e9, Twc);  // EuRoC 형식 (ns)
+        }
+
+        // 다음 프레임까지 대기
+        double T = 0;
+        if(ni < nImages - 1)
+            T = vTimestampsCam[ni+1] - tframe;
+        else if(ni > 0)
+            T = tframe - vTimestampsCam[ni-1];
+
+        if(ttrack < T)
+            usleep((T - ttrack) * 1e6);
+            
+        // 진행 상황 출력
+        if(ni % 100 == 0)
+        {
+            cout << "Processed " << ni << "/" << nImages << " frames" << endl;
+        }
+    }
+
+    // 스레드 종료
+    SLAM.Shutdown();
+    fTrajectory.close();
+
+    // 트래킹 시간 통계
+    sort(vTimesTrack.begin(), vTimesTrack.end());
+    float totaltime = 0;
+    for(int ni=0; ni<nImages; ni++)
+    {
+        totaltime += vTimesTrack[ni];
+    }
+    cout << "-------" << endl << endl;
+    cout << "median tracking time: " << vTimesTrack[nImages/2] << endl;
+    cout << "mean tracking time: " << totaltime/nImages << endl;
+
+    // 최종 궤적 저장
+    SLAM.SaveTrajectoryEuRoC("CameraTrajectory_final.txt");
+    SLAM.SaveKeyFrameTrajectoryEuRoC("KeyFrameTrajectory_final.txt");
+
+    cout << endl << "Trajectory saved to: " << outputFile << endl;
+
+    return 0;
+}
+
+void LoadImages(const string &strPathLeft, const string &strPathRight, const string &strPathTimes,
+                vector<string> &vstrImageLeft, vector<string> &vstrImageRight, vector<double> &vTimeStamps)
+{
+    ifstream fTimes;
+    fTimes.open(strPathTimes.c_str());
+    vTimeStamps.reserve(5000);
+    vstrImageLeft.reserve(5000);
+    vstrImageRight.reserve(5000);
+    
+    while(!fTimes.eof())
+    {
+        string s;
+        getline(fTimes, s);
+        if(!s.empty())
+        {
+            stringstream ss;
+            ss << s;
+            vstrImageLeft.push_back(strPathLeft + "/" + ss.str() + ".png");
+            vstrImageRight.push_back(strPathRight + "/" + ss.str() + ".png");
+            double t;
+            ss >> t;
+            vTimeStamps.push_back(t / 1e9);
+        }
+    }
+}
diff --git a/Examples/Stereo/stereo_realtime.cc b/Examples/Stereo/stereo_realtime.cc
new file mode 100644
index 0000000..ed5bd33
--- /dev/null
+++ b/Examples/Stereo/stereo_realtime.cc
@@ -0,0 +1,356 @@
+/**
+* ORB-SLAM3 실시간 궤적 저장 버전 (범용)
+* 
+* 좌/우 이미지 폴더 경로를 직접 지정할 수 있는 버전
+*
+* 빌드:
+*   CMakeLists.txt에 추가 후 빌드
+* 
+* 사용법:
+*   ./stereo_realtime <vocabulary> <settings> <left_images_dir> <right_images_dir> <timestamps_file> <output_trajectory>
+*
+* 예시:
+*   ./stereo_realtime \
+*       Vocabulary/ORBvoc.txt \
+*       Examples/Stereo/EuRoC.yaml \
+*       /path/to/left_images \
+*       /path/to/right_images \
+*       /path/to/timestamps.txt \
+*       /tmp/realtime_trajectory.txt
+*
+* timestamps.txt 형식:
+*   각 줄에 이미지 파일명(확장자 제외) 또는 타임스탬프
+*   예: 1403636580763555584
+*       1403636580813555456
+*       ...
+*/
+
+#include<iostream>
+#include<algorithm>
+#include<fstream>
+#include<iomanip>
+#include<chrono>
+#include<dirent.h>
+#include<sys/stat.h>
+
+#include<opencv2/core/core.hpp>
+#include<opencv2/imgcodecs.hpp>
+
+#include<System.h>
+
+using namespace std;
+
+// 이미지 확장자 찾기
+string FindImageExtension(const string& directory)
+{
+    DIR* dir = opendir(directory.c_str());
+    if(!dir) return ".png";  // 기본값
+    
+    struct dirent* entry;
+    while((entry = readdir(dir)) != nullptr)
+    {
+        string filename = entry->d_name;
+        if(filename.length() > 4)
+        {
+            string ext = filename.substr(filename.length() - 4);
+            if(ext == ".png" || ext == ".jpg")
+            {
+                closedir(dir);
+                return ext;
+            }
+            if(filename.length() > 5)
+            {
+                ext = filename.substr(filename.length() - 5);
+                if(ext == ".jpeg")
+                {
+                    closedir(dir);
+                    return ext;
+                }
+            }
+        }
+    }
+    closedir(dir);
+    return ".png";
+}
+
+// 파일 존재 확인
+bool FileExists(const string& path)
+{
+    struct stat buffer;
+    return (stat(path.c_str(), &buffer) == 0);
+}
+
+void LoadImages(const string &strPathLeft, const string &strPathRight, const string &strPathTimes,
+                vector<string> &vstrImageLeft, vector<string> &vstrImageRight, vector<double> &vTimeStamps)
+{
+    // 이미지 확장자 자동 탐지
+    string extLeft = FindImageExtension(strPathLeft);
+    string extRight = FindImageExtension(strPathRight);
+    
+    cout << "Detected image extension - Left: " << extLeft << ", Right: " << extRight << endl;
+    
+    ifstream fTimes;
+    fTimes.open(strPathTimes.c_str());
+    
+    if(!fTimes.is_open())
+    {
+        cerr << "ERROR: Cannot open timestamps file: " << strPathTimes << endl;
+        return;
+    }
+    
+    vTimeStamps.reserve(5000);
+    vstrImageLeft.reserve(5000);
+    vstrImageRight.reserve(5000);
+    
+    while(!fTimes.eof())
+    {
+        string s;
+        getline(fTimes, s);
+        
+        // 주석이나 빈 줄 무시
+        if(s.empty() || s[0] == '#')
+            continue;
+            
+        // 공백 제거
+        s.erase(0, s.find_first_not_of(" \t\r\n"));
+        s.erase(s.find_last_not_of(" \t\r\n") + 1);
+        
+        if(s.empty())
+            continue;
+        
+        stringstream ss(s);
+        string imageBaseName;
+        ss >> imageBaseName;
+        
+        // 이미지 경로 구성
+        string leftPath = strPathLeft + "/" + imageBaseName + extLeft;
+        string rightPath = strPathRight + "/" + imageBaseName + extRight;
+        
+        // 파일이 없으면 확장자 없이 시도
+        if(!FileExists(leftPath))
+        {
+            leftPath = strPathLeft + "/" + imageBaseName;
+            if(!FileExists(leftPath))
+            {
+                // 다른 확장자 시도
+                for(const string& ext : {".png", ".jpg", ".jpeg"})
+                {
+                    leftPath = strPathLeft + "/" + imageBaseName + ext;
+                    if(FileExists(leftPath)) break;
+                }
+            }
+        }
+        
+        if(!FileExists(rightPath))
+        {
+            rightPath = strPathRight + "/" + imageBaseName;
+            if(!FileExists(rightPath))
+            {
+                for(const string& ext : {".png", ".jpg", ".jpeg"})
+                {
+                    rightPath = strPathRight + "/" + imageBaseName + ext;
+                    if(FileExists(rightPath)) break;
+                }
+            }
+        }
+        
+        vstrImageLeft.push_back(leftPath);
+        vstrImageRight.push_back(rightPath);
+        
+        // 타임스탬프 파싱 (숫자로 변환)
+        double t;
+        try {
+            t = stod(imageBaseName);
+            // 나노초 단위인 경우 초 단위로 변환
+            if(t > 1e15) t = t / 1e9;
+            else if(t > 1e12) t = t / 1e6;
+            else if(t > 1e9) t = t / 1e3;
+        } catch(...) {
+            t = vTimeStamps.size();  // 숫자가 아닌 경우 인덱스 사용
+        }
+        vTimeStamps.push_back(t);
+    }
+    
+    fTimes.close();
+}
+
+// 실시간 궤적 저장 함수 (TUM 형식)
+void SavePoseRealtime(ofstream &f, double timestamp, const Sophus::SE3f &pose)
+{
+    if(!pose.matrix().isZero(0))
+    {
+        Eigen::Vector3f t = pose.translation();
+        Eigen::Quaternionf q = pose.unit_quaternion();
+        
+        // TUM 형식: timestamp x y z qx qy qz qw
+        f << fixed << setprecision(6) 
+          << timestamp << " "
+          << setprecision(9)
+          << t(0) << " " << t(1) << " " << t(2) << " "
+          << q.x() << " " << q.y() << " " << q.z() << " " << q.w() << endl;
+        f.flush();  // 즉시 파일에 기록
+    }
+}
+
+int main(int argc, char **argv)
+{  
+    if(argc < 7)
+    {
+        cerr << endl << "Usage: ./stereo_realtime <vocabulary> <settings> <left_images_dir> <right_images_dir> <timestamps_file> <output_trajectory>" << endl;
+        cerr << endl << "Arguments:" << endl;
+        cerr << "  vocabulary        : Path to ORB vocabulary file (e.g., Vocabulary/ORBvoc.txt)" << endl;
+        cerr << "  settings          : Path to camera settings YAML file" << endl;
+        cerr << "  left_images_dir   : Directory containing left camera images" << endl;
+        cerr << "  right_images_dir  : Directory containing right camera images" << endl;
+        cerr << "  timestamps_file   : File with image names/timestamps (one per line)" << endl;
+        cerr << "  output_trajectory : Output file for real-time trajectory" << endl;
+        return 1;
+    }
+
+    string pathVoc = string(argv[1]);
+    string pathSettings = string(argv[2]);
+    string pathLeft = string(argv[3]);
+    string pathRight = string(argv[4]);
+    string pathTimes = string(argv[5]);
+    string outputFile = string(argv[6]);
+
+    cout << "=== ORB-SLAM3 Stereo Real-time ===" << endl;
+    cout << "Vocabulary:    " << pathVoc << endl;
+    cout << "Settings:      " << pathSettings << endl;
+    cout << "Left images:   " << pathLeft << endl;
+    cout << "Right images:  " << pathRight << endl;
+    cout << "Timestamps:    " << pathTimes << endl;
+    cout << "Output:        " << outputFile << endl;
+    cout << "==================================" << endl;
+
+    // 이미지 로드
+    vector<string> vstrImageLeft;
+    vector<string> vstrImageRight;
+    vector<double> vTimestampsCam;
+
+    cout << "Loading images..." << endl;
+    LoadImages(pathLeft, pathRight, pathTimes, vstrImageLeft, vstrImageRight, vTimestampsCam);
+    
+    int nImages = vstrImageLeft.size();
+    cout << "Loaded " << nImages << " image pairs" << endl;
+    
+    if(nImages <= 0)
+    {
+        cerr << "ERROR: No images found!" << endl;
+        return 1;
+    }
+    
+    // 첫 번째 이미지 확인
+    cout << "First left image:  " << vstrImageLeft[0] << endl;
+    cout << "First right image: " << vstrImageRight[0] << endl;
+
+    // 실시간 궤적 저장 파일 열기
+    ofstream fTrajectory;
+    fTrajectory.open(outputFile.c_str());
+    fTrajectory << fixed;
+    
+    if(!fTrajectory.is_open())
+    {
+        cerr << "ERROR: Cannot open output file: " << outputFile << endl;
+        return 1;
+    }
+    
+    // 헤더 작성
+    fTrajectory << "# Real-time trajectory from ORB-SLAM3" << endl;
+    fTrajectory << "# Format: timestamp x y z qx qy qz qw" << endl;
+
+    // SLAM 시스템 생성
+    ORB_SLAM3::System SLAM(pathVoc, pathSettings, ORB_SLAM3::System::STEREO, true);
+
+    // 트래킹 시간 통계
+    vector<float> vTimesTrack;
+    vTimesTrack.resize(nImages);
+
+    cout << endl << "Start processing sequence..." << endl;
+
+    cv::Mat imLeft, imRight;
+    int validFrames = 0;
+    
+    for(int ni = 0; ni < nImages; ni++)
+    {
+        // 이미지 로드
+        imLeft = cv::imread(vstrImageLeft[ni], cv::IMREAD_UNCHANGED);
+        imRight = cv::imread(vstrImageRight[ni], cv::IMREAD_UNCHANGED);
+        double tframe = vTimestampsCam[ni];
+
+        if(imLeft.empty())
+        {
+            cerr << "Warning: Failed to load left image: " << vstrImageLeft[ni] << endl;
+            continue;
+        }
+
+        if(imRight.empty())
+        {
+            cerr << "Warning: Failed to load right image: " << vstrImageRight[ni] << endl;
+            continue;
+        }
+
+        // 시간 측정 시작
+        std::chrono::steady_clock::time_point t1 = std::chrono::steady_clock::now();
+
+        // SLAM 트래킹
+        Sophus::SE3f Tcw = SLAM.TrackStereo(imLeft, imRight, tframe, vector<ORB_SLAM3::IMU::Point>(), vstrImageLeft[ni]);
+
+        // 시간 측정 종료
+        std::chrono::steady_clock::time_point t2 = std::chrono::steady_clock::now();
+        double ttrack = std::chrono::duration_cast<std::chrono::duration<double>>(t2 - t1).count();
+        vTimesTrack[ni] = ttrack;
+
+        // 실시간 궤적 저장 (World 좌표계)
+        if(!Tcw.matrix().isZero(0))
+        {
+            Sophus::SE3f Twc = Tcw.inverse();
+            SavePoseRealtime(fTrajectory, tframe, Twc);
+            validFrames++;
+        }
+
+        // 다음 프레임까지 대기
+        double T = 0;
+        if(ni < nImages - 1)
+            T = vTimestampsCam[ni + 1] - tframe;
+        else if(ni > 0)
+            T = tframe - vTimestampsCam[ni - 1];
+
+        if(ttrack < T && T > 0)
+            usleep((T - ttrack) * 1e6);
+            
+        // 진행 상황 출력
+        if(ni % 100 == 0 || ni == nImages - 1)
+        {
+            cout << "\rProcessed " << ni + 1 << "/" << nImages << " frames, valid: " << validFrames << flush;
+        }
+    }
+    
+    cout << endl;
+
+    // 스레드 종료
+    SLAM.Shutdown();
+    fTrajectory.close();
+
+    // 트래킹 시간 통계
+    sort(vTimesTrack.begin(), vTimesTrack.end());
+    float totaltime = 0;
+    for(int ni = 0; ni < nImages; ni++)
+    {
+        totaltime += vTimesTrack[ni];
+    }
+    
+    cout << "-------" << endl;
+    cout << "Total frames: " << nImages << endl;
+    cout << "Valid frames: " << validFrames << endl;
+    cout << "Median tracking time: " << vTimesTrack[nImages / 2] << " s" << endl;
+    cout << "Mean tracking time: " << totaltime / nImages << " s" << endl;
+
+    // 최종 궤적 저장
+    SLAM.SaveTrajectoryEuRoC("CameraTrajectory_final.txt");
+    SLAM.SaveKeyFrameTrajectoryEuRoC("KeyFrameTrajectory_final.txt");
+
+    cout << "Real-time trajectory saved to: " << outputFile << endl;
+
+    return 0;
+}
diff --git a/scripts/realtime_trajectory_plot.py b/scripts/realtime_trajectory_plot.py
new file mode 100644
index 0000000..ee1d7f1
--- /dev/null
+++ b/scripts/realtime_trajectory_plot.py
@@ -0,0 +1,560 @@
+#!/usr/bin/env python3
+"""
+실시간 궤적 시각화 스크립트
+ORB_SLAM3의 추정 궤적과 GT 궤적을 top-down view (x-y plane)로 실시간 플롯
+Sim(3) 정렬 (Umeyama) 기능 포함
+
+사용법:
+    python3 realtime_trajectory_plot.py --gt_file <GT_FILE> --est_file <ESTIMATED_TRAJECTORY_FILE>
+    
+예시 (EuRoC IMU 형식 - 기본값):
+    python3 realtime_trajectory_plot.py \
+        --gt_file /path/to/gt.txt \
+        --est_file /tmp/orb_slam3_trajectory.txt \
+        --format euroc_imu
+
+지원 GT 형식:
+    euroc_imu: timestamp,x,y,z,qw,qx,qy,qz,vx,vy,vz,... (CSV, 쉼표 구분)
+    euroc:     timestamp,x,y,z,qw,qx,qy,qz (CSV 또는 공백 구분)
+    tum:       timestamp x y z qx qy qz qw (공백 구분)
+    kitti:     r00 r01 r02 tx r10 r11 r12 ty r20 r21 r22 tz (변환 행렬)
+
+Sim(3) 정렬:
+    --align 옵션으로 활성화
+    --align_frames: 정렬에 사용할 프레임 수 (기본: 전체)
+"""
+
+import argparse
+import numpy as np
+import matplotlib
+# TkAgg 백엔드 사용 (더 안정적인 실시간 업데이트)
+matplotlib.use('TkAgg')
+import matplotlib.pyplot as plt
+from matplotlib.animation import FuncAnimation
+import os
+import time
+from collections import deque
+import signal
+import sys
+
+
+# Ctrl+C 핸들러
+def signal_handler(sig, frame):
+    print('\nVisualization stopped.')
+    plt.close('all')
+    sys.exit(0)
+
+signal.signal(signal.SIGINT, signal_handler)
+
+
+def umeyama_alignment(src, dst, with_scale=True):
+    """
+    Umeyama 알고리즘을 사용한 Sim(3) 정렬
+    
+    Args:
+        src: 소스 포인트 (N x 3)
+        dst: 대상 포인트 (N x 3)
+        with_scale: 스케일 포함 여부
+        
+    Returns:
+        s: 스케일
+        R: 회전 행렬 (3x3)
+        t: 변환 벡터 (3,)
+    """
+    assert src.shape == dst.shape
+    n, m = src.shape
+    
+    # 평균 계산
+    src_mean = src.mean(axis=0)
+    dst_mean = dst.mean(axis=0)
+    
+    # 평균 빼기
+    src_demean = src - src_mean
+    dst_demean = dst - dst_mean
+    
+    # 공분산 행렬
+    H = src_demean.T @ dst_demean / n
+    
+    # SVD
+    U, S, Vt = np.linalg.svd(H)
+    
+    # 회전 행렬
+    R = Vt.T @ U.T
+    
+    # 반사 처리
+    if np.linalg.det(R) < 0:
+        Vt[-1, :] *= -1
+        R = Vt.T @ U.T
+    
+    # 스케일
+    if with_scale:
+        src_var = np.sum(src_demean ** 2) / n
+        s = np.trace(np.diag(S)) / src_var
+    else:
+        s = 1.0
+    
+    # 변환
+    t = dst_mean - s * R @ src_mean
+    
+    return s, R, t
+
+
+def associate_trajectories(gt_timestamps, est_timestamps, max_diff=0.02):
+    """
+    타임스탬프 기반 궤적 매칭
+    
+    Args:
+        gt_timestamps: GT 타임스탬프 리스트
+        est_timestamps: 추정 궤적 타임스탬프 리스트
+        max_diff: 최대 타임스탬프 차이 (초)
+        
+    Returns:
+        matches: (gt_idx, est_idx) 튜플 리스트
+    """
+    matches = []
+    est_idx = 0
+    
+    for gt_idx, gt_t in enumerate(gt_timestamps):
+        while est_idx < len(est_timestamps) - 1:
+            diff_curr = abs(est_timestamps[est_idx] - gt_t)
+            diff_next = abs(est_timestamps[est_idx + 1] - gt_t)
+            if diff_next < diff_curr:
+                est_idx += 1
+            else:
+                break
+        
+        if est_idx < len(est_timestamps):
+            diff = abs(est_timestamps[est_idx] - gt_t)
+            if diff <= max_diff:
+                matches.append((gt_idx, est_idx))
+    
+    return matches
+
+class TrajectoryPlotter:
+    def __init__(self, gt_file, est_file, format_type='euroc', max_points=10000, update_interval=100,
+                 align=False, align_frames=None):
+        """
+        Args:
+            gt_file: Ground truth 파일 경로
+            est_file: 추정 궤적 파일 경로 (ORB_SLAM3 출력)
+            format_type: 'euroc' 또는 'tum' 또는 'kitti' 또는 'euroc_imu'
+            max_points: 최대 표시할 포인트 수
+            update_interval: 업데이트 간격 (ms)
+            align: Sim(3) 정렬 수행 여부
+            align_frames: 정렬에 사용할 프레임 수 (None이면 전체 사용)
+        """
+        self.gt_file = gt_file
+        self.est_file = est_file
+        self.format_type = format_type
+        self.max_points = max_points
+        self.update_interval = update_interval
+        self.align = align
+        self.align_frames = align_frames
+        
+        # 데이터 저장
+        self.gt_timestamps = []
+        self.gt_x = []
+        self.gt_y = []
+        self.gt_z = []
+        
+        self.est_timestamps = []
+        self.est_x = []
+        self.est_y = []
+        self.est_z = []
+        
+        # 정렬된 데이터
+        self.aligned_x = []
+        self.aligned_y = []
+        self.aligned_z = []
+        
+        # Sim(3) 파라미터
+        self.sim3_scale = None
+        self.sim3_rotation = None
+        self.sim3_translation = None
+        
+        self.last_est_file_size = 0
+        self.last_est_line_count = 0
+        
+        # GT 로드
+        self.load_ground_truth()
+        
+        # Plot 초기화
+        self.setup_plot()
+        
+    def load_ground_truth(self):
+        """GT 파일 로드"""
+        print(f"Loading ground truth from: {self.gt_file}")
+        
+        if not os.path.exists(self.gt_file):
+            print(f"Warning: GT file not found: {self.gt_file}")
+            return
+            
+        with open(self.gt_file, 'r') as f:
+            for line in f:
+                line = line.strip()
+                if line.startswith('#') or len(line) == 0:
+                    continue
+                
+                # CSV 형식 (쉼표 구분) 또는 공백 구분 자동 감지
+                if ',' in line:
+                    parts = [p.strip() for p in line.split(',')]
+                else:
+                    parts = line.split()
+                
+                if self.format_type == 'euroc_imu':
+                    # EuRoC IMU 형식 (사용자 형식):
+                    # timestamp, p_x, p_y, p_z, q_w, q_x, q_y, q_z, v_x, v_y, v_z, ...
+                    if len(parts) >= 8:
+                        try:
+                            timestamp = float(parts[0])
+                            x, y, z = float(parts[1]), float(parts[2]), float(parts[3])
+                            self.gt_timestamps.append(timestamp)
+                            self.gt_x.append(x)
+                            self.gt_y.append(y)
+                            self.gt_z.append(z)
+                        except ValueError:
+                            continue
+                            
+                elif self.format_type == 'euroc':
+                    # EuRoC 형식: timestamp,x,y,z,qw,qx,qy,qz (또는 space-separated)
+                    if len(parts) >= 4:
+                        try:
+                            timestamp = float(parts[0])
+                            x, y, z = float(parts[1]), float(parts[2]), float(parts[3])
+                            self.gt_timestamps.append(timestamp)
+                            self.gt_x.append(x)
+                            self.gt_y.append(y)
+                            self.gt_z.append(z)
+                        except ValueError:
+                            continue
+                            
+                elif self.format_type == 'tum':
+                    # TUM 형식: timestamp x y z qx qy qz qw
+                    if len(parts) >= 4:
+                        try:
+                            timestamp = float(parts[0])
+                            x, y, z = float(parts[1]), float(parts[2]), float(parts[3])
+                            self.gt_timestamps.append(timestamp)
+                            self.gt_x.append(x)
+                            self.gt_y.append(y)
+                            self.gt_z.append(z)
+                        except ValueError:
+                            continue
+                            
+                elif self.format_type == 'kitti':
+                    # KITTI 형식: 12개의 transformation matrix 값 (row-major 3x4)
+                    if len(parts) >= 12:
+                        try:
+                            # r00 r01 r02 tx r10 r11 r12 ty r20 r21 r22 tz
+                            x = float(parts[3])
+                            y = float(parts[7])
+                            z = float(parts[11])
+                            self.gt_timestamps.append(len(self.gt_timestamps))
+                            self.gt_x.append(x)
+                            self.gt_y.append(y)
+                            self.gt_z.append(z)
+                        except ValueError:
+                            continue
+        
+        print(f"Loaded {len(self.gt_timestamps)} GT poses")
+        
+    def load_estimated_trajectory(self):
+        """추정 궤적 파일 실시간 로드"""
+        if not os.path.exists(self.est_file):
+            return False
+            
+        try:
+            current_size = os.path.getsize(self.est_file)
+            if current_size == self.last_est_file_size:
+                return False
+            if current_size == 0:
+                return False
+            self.last_est_file_size = current_size
+        except:
+            return False
+            
+        # 새로운 데이터 읽기
+        self.est_timestamps = []
+        self.est_x = []
+        self.est_y = []
+        self.est_z = []
+        
+        try:
+            with open(self.est_file, 'r') as f:
+                lines = f.readlines()
+                
+            for line in lines:
+                line = line.strip()
+                if line.startswith('#') or len(line) == 0:
+                    continue
+                
+                # 불완전한 라인 스킵 (SLAM이 쓰는 중일 수 있음)
+                if len(line) < 10:
+                    continue
+                    
+                parts = line.split(',') if ',' in line else line.split()
+                
+                # TUM 형식으로 통일 (timestamp x y z qx qy qz qw) - SLAM 출력 형식
+                # euroc_imu, euroc, tum 모두 동일하게 처리 (추정 궤적은 항상 TUM 형식)
+                if len(parts) >= 4:
+                    try:
+                        timestamp = float(parts[0])
+                        x, y, z = float(parts[1]), float(parts[2]), float(parts[3])
+                        self.est_timestamps.append(timestamp)
+                        self.est_x.append(x)
+                        self.est_y.append(y)
+                        self.est_z.append(z)
+                    except ValueError:
+                        continue
+                        
+        except Exception as e:
+            print(f"Error reading estimated trajectory: {e}")
+            return False
+            
+        return True
+    
+    def compute_alignment(self):
+        """
+        Sim(3) 정렬 계산
+        """
+        if len(self.est_timestamps) < 3 or len(self.gt_timestamps) < 3:
+            return False
+        
+        # 타임스탬프 매칭
+        matches = associate_trajectories(self.gt_timestamps, self.est_timestamps)
+        
+        if len(matches) < 3:
+            print(f"Not enough matches for alignment: {len(matches)}")
+            return False
+        
+        # 정렬에 사용할 프레임 수 제한
+        if self.align_frames is not None and len(matches) > self.align_frames:
+            matches = matches[:self.align_frames]
+        
+        # 매칭된 포인트 추출
+        gt_points = np.array([[self.gt_x[gi], self.gt_y[gi], self.gt_z[gi]] 
+                              for gi, ei in matches])
+        est_points = np.array([[self.est_x[ei], self.est_y[ei], self.est_z[ei]] 
+                               for gi, ei in matches])
+        
+        # Umeyama 정렬
+        try:
+            s, R, t = umeyama_alignment(est_points, gt_points, with_scale=True)
+            self.sim3_scale = s
+            self.sim3_rotation = R
+            self.sim3_translation = t
+            return True
+        except Exception as e:
+            print(f"Alignment failed: {e}")
+            return False
+    
+    def apply_alignment(self):
+        """
+        정렬 변환 적용
+        """
+        if self.sim3_scale is None:
+            self.aligned_x = self.est_x.copy()
+            self.aligned_y = self.est_y.copy()
+            self.aligned_z = self.est_z.copy()
+            return
+        
+        self.aligned_x = []
+        self.aligned_y = []
+        self.aligned_z = []
+        
+        for i in range(len(self.est_x)):
+            p = np.array([self.est_x[i], self.est_y[i], self.est_z[i]])
+            p_aligned = self.sim3_scale * self.sim3_rotation @ p + self.sim3_translation
+            self.aligned_x.append(p_aligned[0])
+            self.aligned_y.append(p_aligned[1])
+            self.aligned_z.append(p_aligned[2])
+        
+    def setup_plot(self):
+        """Plot 초기화"""
+        plt.ion()
+        self.fig, self.axes = plt.subplots(1, 2, figsize=(14, 6))
+        
+        # Top-down view (X-Y)
+        self.ax_xy = self.axes[0]
+        self.ax_xy.set_xlabel('X [m]')
+        self.ax_xy.set_ylabel('Y [m]')
+        self.ax_xy.set_title('Top-Down View (X-Y Plane)')
+        self.ax_xy.grid(True, alpha=0.3)
+        self.ax_xy.set_aspect('equal')
+        
+        # Side view (X-Z)
+        self.ax_xz = self.axes[1]
+        self.ax_xz.set_xlabel('X [m]')
+        self.ax_xz.set_ylabel('Z [m]')
+        self.ax_xz.set_title('Side View (X-Z Plane)')
+        self.ax_xz.grid(True, alpha=0.3)
+        self.ax_xz.set_aspect('equal')
+        
+        # GT 라인
+        self.gt_line_xy, = self.ax_xy.plot([], [], 'b-', linewidth=1.5, label='Ground Truth', alpha=0.7)
+        self.gt_line_xz, = self.ax_xz.plot([], [], 'b-', linewidth=1.5, label='Ground Truth', alpha=0.7)
+        
+        # 추정 라인
+        self.est_line_xy, = self.ax_xy.plot([], [], 'r-', linewidth=1.5, label='Estimated', alpha=0.7)
+        self.est_line_xz, = self.ax_xz.plot([], [], 'r-', linewidth=1.5, label='Estimated', alpha=0.7)
+        
+        # 현재 위치 마커
+        self.gt_marker_xy, = self.ax_xy.plot([], [], 'bo', markersize=8)
+        self.est_marker_xy, = self.ax_xy.plot([], [], 'ro', markersize=8)
+        self.gt_marker_xz, = self.ax_xz.plot([], [], 'bo', markersize=8)
+        self.est_marker_xz, = self.ax_xz.plot([], [], 'ro', markersize=8)
+        
+        # 시작점 마커
+        self.start_marker_xy, = self.ax_xy.plot([], [], 'g^', markersize=10, label='Start')
+        self.start_marker_xz, = self.ax_xz.plot([], [], 'g^', markersize=10, label='Start')
+        
+        self.ax_xy.legend(loc='upper left')
+        self.ax_xz.legend(loc='upper left')
+        
+        # 정보 텍스트
+        self.info_text = self.fig.text(0.5, 0.02, '', ha='center', fontsize=10)
+        
+        plt.tight_layout()
+        plt.subplots_adjust(bottom=0.1)
+        
+    def update_plot(self, frame):
+        """Plot 업데이트"""
+        # 추정 궤적 갱신
+        data_updated = self.load_estimated_trajectory()
+        
+        # 정렬 수행
+        if self.align and data_updated and len(self.est_x) > 10:
+            self.compute_alignment()
+            self.apply_alignment()
+        elif not self.align:
+            self.aligned_x = self.est_x.copy()
+            self.aligned_y = self.est_y.copy()
+            self.aligned_z = self.est_z.copy()
+        
+        # GT 업데이트
+        if len(self.gt_x) > 0:
+            self.gt_line_xy.set_data(self.gt_x, self.gt_y)
+            self.gt_line_xz.set_data(self.gt_x, self.gt_z)
+            self.gt_marker_xy.set_data([self.gt_x[-1]], [self.gt_y[-1]])
+            self.gt_marker_xz.set_data([self.gt_x[-1]], [self.gt_z[-1]])
+            
+            if len(self.gt_x) > 0:
+                self.start_marker_xy.set_data([self.gt_x[0]], [self.gt_y[0]])
+                self.start_marker_xz.set_data([self.gt_x[0]], [self.gt_z[0]])
+        
+        # 추정 궤적 업데이트 (정렬된 데이터 사용)
+        if len(self.aligned_x) > 0:
+            self.est_line_xy.set_data(self.aligned_x, self.aligned_y)
+            self.est_line_xz.set_data(self.aligned_x, self.aligned_z)
+            self.est_marker_xy.set_data([self.aligned_x[-1]], [self.aligned_y[-1]])
+            self.est_marker_xz.set_data([self.aligned_x[-1]], [self.aligned_z[-1]])
+        
+        # 축 범위 자동 조정
+        all_x = self.gt_x + self.aligned_x
+        all_y = self.gt_y + self.aligned_y
+        all_z = self.gt_z + self.aligned_z
+        
+        if len(all_x) > 0:
+            margin = 2.0
+            
+            x_min, x_max = min(all_x) - margin, max(all_x) + margin
+            y_min, y_max = min(all_y) - margin, max(all_y) + margin
+            z_min, z_max = min(all_z) - margin, max(all_z) + margin
+            
+            # 동일한 스케일 유지
+            xy_range = max(x_max - x_min, y_max - y_min)
+            xz_range = max(x_max - x_min, z_max - z_min)
+            
+            x_center = (x_min + x_max) / 2
+            y_center = (y_min + y_max) / 2
+            z_center = (z_min + z_max) / 2
+            
+            self.ax_xy.set_xlim(x_center - xy_range/2, x_center + xy_range/2)
+            self.ax_xy.set_ylim(y_center - xy_range/2, y_center + xy_range/2)
+            
+            self.ax_xz.set_xlim(x_center - xz_range/2, x_center + xz_range/2)
+            self.ax_xz.set_ylim(z_center - xz_range/2, z_center + xz_range/2)
+        
+        # 정보 업데이트
+        info = f"GT poses: {len(self.gt_x)} | Estimated poses: {len(self.est_x)}"
+        if self.align and self.sim3_scale is not None:
+            info += f" | Scale: {self.sim3_scale:.4f}"
+        if len(self.aligned_x) > 0:
+            info += f" | Aligned pos: ({self.aligned_x[-1]:.2f}, {self.aligned_y[-1]:.2f}, {self.aligned_z[-1]:.2f})"
+        self.info_text.set_text(info)
+        
+        return [self.gt_line_xy, self.gt_line_xz, self.est_line_xy, self.est_line_xz,
+                self.gt_marker_xy, self.gt_marker_xz, self.est_marker_xy, self.est_marker_xz,
+                self.start_marker_xy, self.start_marker_xz, self.info_text]
+    
+    def run(self):
+        """실시간 플롯 실행"""
+        print("="*60)
+        print("Starting real-time trajectory visualization...")
+        print(f"Monitoring estimated trajectory file: {self.est_file}")
+        print("="*60)
+        
+        # 파일이 생성될 때까지 대기
+        if not os.path.exists(self.est_file):
+            print(f"Waiting for trajectory file to be created...")
+            print("(Start SLAM to begin generating trajectory)")
+            while not os.path.exists(self.est_file):
+                plt.pause(0.5)  # GUI 이벤트 처리하면서 대기
+                if not plt.fignum_exists(self.fig.number):
+                    print("Window closed. Exiting.")
+                    return
+            print(f"File detected: {self.est_file}")
+        
+        # 파일에 데이터가 쓰여질 때까지 대기
+        print("Waiting for trajectory data...")
+        while True:
+            try:
+                if os.path.getsize(self.est_file) > 0:
+                    break
+            except:
+                pass
+            plt.pause(0.5)
+            if not plt.fignum_exists(self.fig.number):
+                print("Window closed. Exiting.")
+                return
+        
+        print("Trajectory data detected. Starting visualization...")
+        print("Press Ctrl+C or close the window to stop")
+        print("="*60)
+        
+        ani = FuncAnimation(self.fig, self.update_plot, interval=self.update_interval, 
+                           blit=False, cache_frame_data=False)
+        plt.show(block=True)
+
+
+def main():
+    parser = argparse.ArgumentParser(description='Real-time trajectory visualization for ORB_SLAM3')
+    parser.add_argument('--gt_file', type=str, required=True,
+                        help='Path to ground truth file')
+    parser.add_argument('--est_file', type=str, required=True,
+                        help='Path to estimated trajectory file (will be monitored)')
+    parser.add_argument('--format', type=str, default='euroc_imu', choices=['euroc', 'euroc_imu', 'tum', 'kitti'],
+                        help='Trajectory file format (default: euroc_imu)')
+    parser.add_argument('--interval', type=int, default=100,
+                        help='Update interval in milliseconds (default: 100)')
+    parser.add_argument('--align', action='store_true',
+                        help='Enable Sim(3) alignment (Umeyama algorithm)')
+    parser.add_argument('--align_frames', type=int, default=None,
+                        help='Number of frames to use for alignment (default: all)')
+    
+    args = parser.parse_args()
+    
+    plotter = TrajectoryPlotter(
+        gt_file=args.gt_file,
+        est_file=args.est_file,
+        format_type=args.format,
+        update_interval=args.interval,
+        align=args.align,
+        align_frames=args.align_frames
+    )
+    
+    plotter.run()
+
+
+if __name__ == '__main__':
+    main()
diff --git a/scripts/run_slam_custom_dataset.sh b/scripts/run_slam_custom_dataset.sh
new file mode 100755
index 0000000..c61bdc8
--- /dev/null
+++ b/scripts/run_slam_custom_dataset.sh
@@ -0,0 +1,103 @@
+#!/bin/bash
+# SLAM for new dataset format with real-time visualization
+#
+# Usage:
+#   ./run_slam_custom_dataset.sh <vocab> <settings> <dataset_path> [output_traj]
+#
+# Example:
+#   ./run_slam_custom_dataset.sh \
+#       Vocabulary/ORBvoc.txt \
+#       Examples/Stereo/lunar.yaml \
+#       ./dataset
+
+SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
+ORB_SLAM3_DIR="$(dirname "$SCRIPT_DIR")"
+
+if [ "$#" -lt 2 ]; then
+    echo "Usage: $0 <dataset_path> [output_traj]"
+    echo ""
+    echo "Dataset structure:"
+    echo "  dataset_path/"
+    echo "    frame_index.txt"
+    echo "    data/"
+    echo "      left_*.png"
+    echo "      right_*.png"
+    echo "      tf_*.txt"
+    echo ""
+    echo "Example:"
+    echo "  $0 ./dataset"
+    exit 1
+fi
+
+VOCAB="Vocabulary/ORBvoc.txt"
+SETTINGS="Examples/Stereo/CustomDataset.yaml"
+DATASET_PATH="$1"
+OUTPUT_TRAJ="${2:-estimated_trajectory.txt}"
+
+# Store current directory
+CURRENT_DIR="$(pwd)"
+
+# Convert relative paths to absolute paths
+VOCAB=$(realpath "$VOCAB" 2>/dev/null || echo "$CURRENT_DIR/$VOCAB")
+SETTINGS=$(realpath "$SETTINGS" 2>/dev/null || echo "$CURRENT_DIR/$SETTINGS")
+DATASET_PATH=$(realpath "$DATASET_PATH" 2>/dev/null || echo "$CURRENT_DIR/$DATASET_PATH")
+
+# Change to ORB_SLAM3 directory
+cd "$ORB_SLAM3_DIR"
+
+# Make output trajectory absolute path
+if [[ "$OUTPUT_TRAJ" != /* ]]; then
+    OUTPUT_TRAJ="$ORB_SLAM3_DIR/$OUTPUT_TRAJ"
+fi
+
+# Remove previous trajectory files
+rm -f "$OUTPUT_TRAJ"
+
+echo "=============================================="
+echo "Starting ORB-SLAM3 with Custom Dataset Format"
+echo "=============================================="
+echo "Vocabulary: $VOCAB"
+echo "Settings: $SETTINGS"
+echo "Dataset path: $DATASET_PATH"
+echo "Output trajectory: $OUTPUT_TRAJ"
+echo "=============================================="
+
+# Start visualization in background
+GT_FILE="$DATASET_PATH/gt_trajectory.txt"
+echo "Starting visualization..."
+python3 "$SCRIPT_DIR/realtime_trajectory_plot.py" \
+    --gt_file "$GT_FILE" \
+    --est_file "$OUTPUT_TRAJ" \
+    --format tum \
+    --interval 200 &
+
+VIZ_PID=$!
+echo "Visualization PID: $VIZ_PID"
+
+# Wait for visualization window to appear
+sleep 2
+
+# Run SLAM
+echo "Starting SLAM..."
+"$ORB_SLAM3_DIR/Examples/Stereo/stereo_custom_dataset" \
+    "$VOCAB" \
+    "$SETTINGS" \
+    "$DATASET_PATH" \
+    "$OUTPUT_TRAJ"
+
+SLAM_EXIT=$?
+
+# SLAM finished
+echo ""
+echo "SLAM finished with exit code: $SLAM_EXIT"
+echo "Press Enter to close visualization or wait 10 seconds..."
+
+# Wait and close visualization
+sleep 10
+kill $VIZ_PID 2>/dev/null
+
+echo "Done."
+echo ""
+echo "Output files:"
+echo "  - Estimated trajectory: $OUTPUT_TRAJ"
+echo "  - GT trajectory: $GT_FILE"
diff --git a/scripts/run_slam_with_viz.sh b/scripts/run_slam_with_viz.sh
new file mode 100755
index 0000000..8b19f93
--- /dev/null
+++ b/scripts/run_slam_with_viz.sh
@@ -0,0 +1,99 @@
+#!/bin/bash
+# SLAM과 실시간 시각화를 동시에 실행하는 스크립트
+#
+# 사용법:
+#   ./run_slam_with_viz.sh <vocab> <settings> <left_dir> <right_dir> <gt_file>
+#
+# 예시:
+#   ./run_slam_with_viz.sh \
+#       Vocabulary/ORBvoc.txt \
+#       Examples/Stereo/lunar.yaml \
+#       /path/to/left \
+#       /path/to/right \
+#       /path/to/gt.txt
+
+SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
+ORB_SLAM3_DIR="$(dirname "$SCRIPT_DIR")"
+
+if [ "$#" -lt 5 ]; then
+    echo "Usage: $0 <vocab> <settings> <left_dir> <right_dir> <gt_file> [output_traj]"
+    echo ""
+    echo "Example:"
+    echo "  $0 Vocabulary/ORBvoc.txt Examples/Stereo/lunar.yaml /data/left /data/right /data/gt.txt"
+    exit 1
+fi
+
+VOCAB="$1"
+SETTINGS="$2"
+LEFT_DIR="$3"
+RIGHT_DIR="$4"
+GT_FILE="$5"
+OUTPUT_TRAJ="${6:-estimated_trajectory.txt}"
+
+# 현재 디렉토리 저장 (스크립트 실행 위치)
+CURRENT_DIR="$(pwd)"
+
+# 먼저 상대 경로를 절대 경로로 변환 (현재 디렉토리 기준)
+VOCAB=$(realpath "$VOCAB" 2>/dev/null || echo "$CURRENT_DIR/$VOCAB")
+SETTINGS=$(realpath "$SETTINGS" 2>/dev/null || echo "$CURRENT_DIR/$SETTINGS")
+LEFT_DIR=$(realpath "$LEFT_DIR" 2>/dev/null || echo "$CURRENT_DIR/$LEFT_DIR")
+RIGHT_DIR=$(realpath "$RIGHT_DIR" 2>/dev/null || echo "$CURRENT_DIR/$RIGHT_DIR")
+GT_FILE=$(realpath "$GT_FILE" 2>/dev/null || echo "$CURRENT_DIR/$GT_FILE")
+
+# ORB_SLAM3 디렉토리로 이동
+cd "$ORB_SLAM3_DIR"
+
+# Output trajectory도 절대 경로로
+if [[ "$OUTPUT_TRAJ" != /* ]]; then
+    OUTPUT_TRAJ="$ORB_SLAM3_DIR/$OUTPUT_TRAJ"
+fi
+
+# 이전 trajectory 파일 삭제
+rm -f "$OUTPUT_TRAJ"
+
+echo "=============================================="
+echo "Starting ORB-SLAM3 with Real-time Visualization"
+echo "=============================================="
+echo "Vocabulary: $VOCAB"
+echo "Settings: $SETTINGS"
+echo "Left images: $LEFT_DIR"
+echo "Right images: $RIGHT_DIR"
+echo "GT file: $GT_FILE"
+echo "Output trajectory: $OUTPUT_TRAJ"
+echo "=============================================="
+
+# 시각화 스크립트를 백그라운드에서 먼저 시작
+echo "Starting visualization..."
+python3 "$SCRIPT_DIR/realtime_trajectory_plot.py" \
+    --gt_file "$GT_FILE" \
+    --est_file "$OUTPUT_TRAJ" \
+    --format euroc_imu \
+    --interval 200 &
+
+VIZ_PID=$!
+echo "Visualization PID: $VIZ_PID"
+
+# 잠시 대기하여 시각화 창이 뜨도록 함
+sleep 2
+
+# SLAM 실행
+echo "Starting SLAM..."
+"$ORB_SLAM3_DIR/Examples/Stereo/stereo_custom" \
+    "$VOCAB" \
+    "$SETTINGS" \
+    "$LEFT_DIR" \
+    "$RIGHT_DIR" \
+    "$OUTPUT_TRAJ"
+
+SLAM_EXIT=$?
+
+# SLAM이 종료되면 시각화도 종료
+echo ""
+echo "SLAM finished with exit code: $SLAM_EXIT"
+echo "Press Enter to close visualization or wait 10 seconds..."
+
+# 10초 대기 후 시각화 종료
+sleep 10
+kill $VIZ_PID 2>/dev/null
+
+echo "Done."
diff --git a/scripts/run_stereo_with_visualization.py b/scripts/run_stereo_with_visualization.py
new file mode 100644
index 0000000..241c3ed
--- /dev/null
+++ b/scripts/run_stereo_with_visualization.py
@@ -0,0 +1,156 @@
+#!/usr/bin/env python3
+"""
+ORB_SLAM3 실행과 동시에 실시간 궤적 시각화를 위한 래퍼 스크립트
+
+사용법:
+    python3 run_stereo_with_visualization.py \
+        --left_images <LEFT_DIR> \
+        --right_images <RIGHT_DIR> \
+        --timestamps <TIMESTAMPS_FILE> \
+        --gt_file <GT_FILE> \
+        --settings <SETTINGS_YAML>
+
+예시:
+    python3 run_stereo_with_visualization.py \
+        --left_images /data/stereo/left \
+        --right_images /data/stereo/right \
+        --timestamps /data/stereo/timestamps.txt \
+        --gt_file /data/stereo/groundtruth.txt \
+        --settings ../Examples/Stereo/EuRoC.yaml
+"""
+
+import argparse
+import subprocess
+import os
+import sys
+import time
+import signal
+import threading
+from pathlib import Path
+
+# 현재 스크립트 디렉토리 기준 상대 경로
+SCRIPT_DIR = Path(__file__).parent.absolute()
+PROJECT_ROOT = SCRIPT_DIR.parent
+
+
+def run_orbslam3(vocab, settings, left_dir, right_dir, timestamps, output_file, executable):
+    """ORB_SLAM3 실행"""
+    cmd = [
+        str(executable),
+        str(vocab),
+        str(settings),
+        str(left_dir),
+        str(right_dir),
+        str(timestamps),
+        str(output_file)
+    ]
+    
+    print(f"Running ORB_SLAM3: {' '.join(cmd)}")
+    
+    process = subprocess.Popen(
+        cmd,
+        stdout=subprocess.PIPE,
+        stderr=subprocess.STDOUT,
+        text=True,
+        bufsize=1
+    )
+    
+    # 출력 스트리밍
+    for line in iter(process.stdout.readline, ''):
+        print(f"[ORB_SLAM3] {line}", end='')
+    
+    process.wait()
+    return process.returncode
+
+
+def run_visualization(gt_file, est_file, format_type):
+    """시각화 스크립트 실행"""
+    viz_script = SCRIPT_DIR / 'realtime_trajectory_plot.py'
+    
+    cmd = [
+        sys.executable,
+        str(viz_script),
+        '--gt_file', str(gt_file),
+        '--est_file', str(est_file),
+        '--format', format_type
+    ]
+    
+    print(f"Running visualization: {' '.join(cmd)}")
+    
+    process = subprocess.Popen(cmd)
+    return process
+
+
+def main():
+    parser = argparse.ArgumentParser(description='Run ORB_SLAM3 with real-time visualization')
+    
+    parser.add_argument('--vocabulary', type=str, 
+                        default=str(PROJECT_ROOT / 'Vocabulary/ORBvoc.txt'),
+                        help='Path to vocabulary file')
+    parser.add_argument('--settings', type=str, required=True,
+                        help='Path to settings YAML file')
+    parser.add_argument('--left_images', type=str, required=True,
+                        help='Directory containing left camera images')
+    parser.add_argument('--right_images', type=str, required=True,
+                        help='Directory containing right camera images')
+    parser.add_argument('--timestamps', type=str, required=True,
+                        help='Path to timestamps file')
+    parser.add_argument('--gt_file', type=str, required=True,
+                        help='Path to ground truth file')
+    parser.add_argument('--output', type=str, default='/tmp/orb_slam3_trajectory.txt',
+                        help='Output trajectory file path')
+    parser.add_argument('--format', type=str, default='euroc_imu', 
+                        choices=['euroc', 'euroc_imu', 'tum', 'kitti'],
+                        help='GT trajectory format (default: euroc_imu)')
+    parser.add_argument('--executable', type=str,
+                        default=str(PROJECT_ROOT / 'Examples/Stereo/stereo_realtime'),
+                        help='Path to ORB_SLAM3 executable')
+    
+    args = parser.parse_args()
+    
+    # 경로 확인
+    for path, name in [(args.left_images, 'Left images'), 
+                       (args.right_images, 'Right images'),
+                       (args.timestamps, 'Timestamps'),
+                       (args.gt_file, 'GT file'),
+                       (args.settings, 'Settings')]:
+        if not os.path.exists(path):
+            print(f"ERROR: {name} not found: {path}")
+            return 1
+    
+    # 출력 파일 초기화
+    output_file = Path(args.output)
+    if output_file.exists():
+        output_file.unlink()
+    output_file.touch()
+    
+    # 시각화 먼저 시작 (백그라운드)
+    viz_process = run_visualization(args.gt_file, str(output_file), args.format)
+    time.sleep(2)  # 시각화 창이 뜰 때까지 대기
+    
+    try:
+        # ORB_SLAM3 실행
+        return_code = run_orbslam3(
+            args.vocabulary,
+            args.settings,
+            args.left_images,
+            args.right_images,
+            args.timestamps,
+            str(output_file),
+            args.executable
+        )
+        
+        print(f"\nORB_SLAM3 finished with return code: {return_code}")
+        print("Visualization window is still open. Close it to exit.")
+        
+        viz_process.wait()
+        
+    except KeyboardInterrupt:
+        print("\nInterrupted by user")
+        viz_process.terminate()
+    
+    return 0
+
+
+if __name__ == '__main__':
+    sys.exit(main())
